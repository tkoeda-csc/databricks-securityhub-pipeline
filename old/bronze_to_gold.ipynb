{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f907c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog_name\", \"cloudfastener\")\n",
    "dbutils.widgets.text(\"company_id\", \"\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "company_id_param = dbutils.widgets.get(\"company_id\").strip()\n",
    "\n",
    "if not catalog_name:\n",
    "    raise ValueError(\"Missing required param: catalog_name\")\n",
    "\n",
    "# Set timezone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# Calculate job date and processing window\n",
    "# job_date = current day at 00:00:00 UTC\n",
    "# window = [job_date - 1 day, job_date]\n",
    "job_date = F.date_trunc(\"DAY\", F.current_timestamp())\n",
    "window_end_ts = F.to_timestamp(job_date)\n",
    "window_start_ts = window_end_ts - F.expr(\"INTERVAL 1 DAY\")\n",
    "\n",
    "# cf_processed_time for data records (same as job_date)\n",
    "cf_processed_time = job_date\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BRONZE → SILVER → GOLD ETL Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"catalog_name     = {catalog_name}\")\n",
    "print(f\"job_date         = {job_date}\")\n",
    "print(f\"window_start     = window_end - 1 day\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f520d78",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_company_id(schema_name: str) -> bool:\n",
    "    \"\"\"Check if schema name matches company ID format: 12 chars, lowercase alphanumeric.\"\"\"\n",
    "    return (\n",
    "        len(schema_name) == 12 and\n",
    "        schema_name.isalnum() and\n",
    "        schema_name.islower()\n",
    "    )\n",
    "\n",
    "def discover_companies(catalog: str) -> list:\n",
    "    \"\"\"Discover all company schemas in the catalog.\"\"\"\n",
    "    try:\n",
    "        databases = spark.catalog.listDatabases()\n",
    "        companies = []\n",
    "        for db in databases:\n",
    "            # Schema format: catalog.company_id or just company_id\n",
    "            # Extract the last part after splitting by '.'\n",
    "            parts = db.name.split('.')\n",
    "            # Handle both formats: \"cloudfastener.xs22xw4aw73q\" or \"xs22xw4aw73q\"\n",
    "            if len(parts) >= 2 and parts[0] == catalog:\n",
    "                schema_name = parts[1]\n",
    "            elif len(parts) == 1:\n",
    "                schema_name = parts[0]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if is_valid_company_id(schema_name):\n",
    "                companies.append(schema_name)\n",
    "        return sorted(companies)\n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering companies: {e}\")\n",
    "        return []\n",
    "\n",
    "def table_exists(full_name: str) -> bool:\n",
    "    \"\"\"Check if a table exists in the catalog.\"\"\"\n",
    "    try:\n",
    "        return spark.catalog.tableExists(full_name)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalize_finding_id(col):\n",
    "    \"\"\"Normalize finding ID: trim and convert empty to NULL.\"\"\"\n",
    "    return F.when(F.length(F.trim(col)) == 0, F.lit(None)).otherwise(F.trim(col))\n",
    "\n",
    "def parse_iso8601_to_ts(col):\n",
    "    \"\"\"Parse ISO8601 timestamp string to Spark timestamp.\"\"\"\n",
    "    return F.to_timestamp(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819f9ed",
   "metadata": {},
   "source": [
    "## Discover Companies to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine companies to process\n",
    "if not company_id_param or company_id_param.upper() == \"ALL\":\n",
    "    companies_to_process = discover_companies(catalog_name)\n",
    "    print(f\"Auto-discovery mode: Found {len(companies_to_process)} companies\")\n",
    "    if companies_to_process:\n",
    "        print(f\"Companies: {', '.join(companies_to_process)}\")\n",
    "else:\n",
    "    # Single company mode\n",
    "    if not is_valid_company_id(company_id_param):\n",
    "        raise ValueError(f\"Invalid company_id format: {company_id_param}. Must be 12 lowercase alphanumeric characters.\")\n",
    "    companies_to_process = [company_id_param]\n",
    "    print(f\"Single company mode: {company_id_param}\")\n",
    "\n",
    "if not companies_to_process:\n",
    "    raise ValueError(\"No companies to process. Check catalog and schema names.\")\n",
    "\n",
    "print(f\"\\nTotal companies to process: {len(companies_to_process)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678ace6",
   "metadata": {},
   "source": [
    "## Process Each Company\n",
    "\n",
    "Loop through each company and run the bronze → silver → gold pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab52791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results\n",
    "successful_companies = []\n",
    "failed_companies = []\n",
    "skipped_companies = []\n",
    "\n",
    "for company_id in companies_to_process:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Processing company: {company_id}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # Define table names for this company\n",
    "        asff_tbl = f\"{catalog_name}.{company_id}.aws_securityhub_findings_1_0\"\n",
    "        ocsf_tbl = f\"{catalog_name}.{company_id}.aws_securitylake_sh_findings_2_0\"\n",
    "        silver_tbl = f\"{catalog_name}.{company_id}.aws_compliance_findings\"\n",
    "        gold_tbl = f\"{catalog_name}.{company_id}.aws_standard_summary\"\n",
    "\n",
    "        print(f\"ASFF bronze      = {asff_tbl}\")\n",
    "        print(f\"OCSF bronze      = {ocsf_tbl}\")\n",
    "        print(f\"Silver           = {silver_tbl}\")\n",
    "        print(f\"Gold             = {gold_tbl}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # ============================================================\n",
    "        # BRONZE → SILVER: Load and Transform\n",
    "        # ============================================================\n",
    "\n",
    "        # Check table existence\n",
    "        asff_exists = table_exists(asff_tbl)\n",
    "        ocsf_exists = table_exists(ocsf_tbl)\n",
    "\n",
    "        print(f\"ASFF exists? {asff_exists}\")\n",
    "        print(f\"OCSF exists? {ocsf_exists}\")\n",
    "\n",
    "        if not asff_exists and not ocsf_exists:\n",
    "            print(f\"⚠️  Neither bronze table exists for {company_id}. Skipping...\")\n",
    "            skipped_companies.append((company_id, \"No bronze tables\"))\n",
    "            continue\n",
    "\n",
    "        sources = []\n",
    "\n",
    "        # Load ASFF data\n",
    "        if asff_exists:\n",
    "            df_asff_raw = (\n",
    "                spark.table(asff_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"product_name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            asff_count = df_asff_raw.count()\n",
    "            print(f\"ASFF rows in window: {asff_count}\")\n",
    "            if asff_count > 0:\n",
    "                sources.append((\"ASFF\", df_asff_raw))\n",
    "\n",
    "        # Load OCSF data\n",
    "        if ocsf_exists:\n",
    "            df_ocsf_raw = (\n",
    "                spark.table(ocsf_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"metadata.product.name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            ocsf_count = df_ocsf_raw.count()\n",
    "            print(f\"OCSF rows in window: {ocsf_count}\")\n",
    "            if ocsf_count > 0:\n",
    "                sources.append((\"OCSF\", df_ocsf_raw))\n",
    "\n",
    "        if len(sources) == 0:\n",
    "            print(f\"⚠️  No rows found in window for {company_id}. Skipping...\")\n",
    "            skipped_companies.append((company_id, \"No data in window\"))\n",
    "            continue\n",
    "\n",
    "        # Transform Functions\n",
    "        def transform_asff(df):\n",
    "            return (\n",
    "                df.select(\n",
    "                    normalize_finding_id(F.col(\"finding_id\")).alias(\"finding_id\"),\n",
    "                    parse_iso8601_to_ts(F.col(\"updated_at\")).alias(\"finding_modified_time\"),\n",
    "                    F.to_timestamp(F.lit(cf_processed_time)).alias(\"cf_processed_time\"),\n",
    "                    F.when(F.upper(F.col(\"workflow.Status\")) == \"NEW\", \"New\")\n",
    "                     .when(F.upper(F.col(\"workflow.Status\")) == \"NOTIFIED\", \"In Progress\")\n",
    "                     .when(F.upper(F.col(\"workflow.Status\")) == \"SUPPRESSED\", \"Suppressed\")\n",
    "                     .when(F.upper(F.col(\"workflow.Status\")) == \"RESOLVED\", \"Resolved\")\n",
    "                     .otherwise(F.col(\"workflow.Status\"))\n",
    "                     .alias(\"finding_status\"),\n",
    "                    F.col(\"aws_account_id\").cast(\"string\").alias(\"account_id\"),\n",
    "                    F.col(\"finding_region\").cast(\"string\").alias(\"region_id\"),\n",
    "                    F.expr(\"compliance.AssociatedStandards[0].StandardsId\").cast(\"string\").alias(\"standard_id\"),\n",
    "                    F.col(\"compliance.SecurityControlId\").cast(\"string\").alias(\"control_id\"),\n",
    "                    F.col(\"compliance.Status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "                    F.col(\"severity.Label\").cast(\"string\").alias(\"severity\"),\n",
    "                    F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "                    F.lit(1).alias(\"_preference\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        def transform_ocsf(df):\n",
    "            return (\n",
    "                df.select(\n",
    "                    normalize_finding_id(F.col(\"finding_info.uid\")).alias(\"finding_id\"),\n",
    "                    parse_iso8601_to_ts(F.col(\"finding_info.modified_time_dt\")).alias(\"finding_modified_time\"),\n",
    "                    F.to_timestamp(F.lit(cf_processed_time)).alias(\"cf_processed_time\"),\n",
    "                    F.col(\"status\").cast(\"string\").alias(\"finding_status\"),\n",
    "                    F.col(\"cloud.account.uid\").cast(\"string\").alias(\"account_id\"),\n",
    "                    F.col(\"cloud.region\").cast(\"string\").alias(\"region_id\"),\n",
    "                    F.expr(\"compliance.standards[0]\").cast(\"string\").alias(\"standard_id\"),\n",
    "                    F.col(\"compliance.control\").cast(\"string\").alias(\"control_id\"),\n",
    "                    F.col(\"compliance.status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "                    F.col(\"severity\").cast(\"string\").alias(\"severity\"),\n",
    "                    F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "                    F.lit(0).alias(\"_preference\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Transform and union\n",
    "        canonical_dfs = []\n",
    "        for src, df_raw in sources:\n",
    "            if src == \"ASFF\":\n",
    "                out = transform_asff(df_raw)\n",
    "            elif src == \"OCSF\":\n",
    "                out = transform_ocsf(df_raw)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            out = out.withColumn(\"finding_id\", normalize_finding_id(F.col(\"finding_id\"))) \\\n",
    "                     .where(F.col(\"finding_id\").isNotNull())\n",
    "            canonical_dfs.append(out)\n",
    "\n",
    "        if not canonical_dfs:\n",
    "            print(f\"⚠️  No valid findings after filtering for {company_id}. Skipping...\")\n",
    "            skipped_companies.append((company_id, \"No valid findings\"))\n",
    "            continue\n",
    "\n",
    "        df_union = canonical_dfs[0]\n",
    "        for d in canonical_dfs[1:]:\n",
    "            df_union = df_union.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "        print(f\"Union rows: {df_union.count()}\")\n",
    "\n",
    "        # Deduplicate\n",
    "        w = Window.partitionBy(\"finding_id\").orderBy(\n",
    "            F.col(\"finding_modified_time\").desc_nulls_last(),\n",
    "            F.col(\"_preference\").desc(),\n",
    "            F.col(\"_bronze_processed_time\").desc_nulls_last()\n",
    "        )\n",
    "\n",
    "        df_winners = (\n",
    "            df_union\n",
    "            .withColumn(\"_rn\", F.row_number().over(w))\n",
    "            .where(F.col(\"_rn\") == 1)\n",
    "        )\n",
    "\n",
    "        winner_count = df_winners.count()\n",
    "        print(f\"Winners after dedup: {winner_count} rows\")\n",
    "\n",
    "        # Prepare for merge (drop internal columns including _preference)\n",
    "        df_stage = (\n",
    "            df_winners\n",
    "            .drop(\"_rn\", \"_preference\", \"_bronze_processed_time\")\n",
    "        )\n",
    "\n",
    "        df_stage.createOrReplaceTempView(\"stg_silver\")\n",
    "\n",
    "        # MERGE into silver (no source_preference column)\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO {silver_tbl} t\n",
    "            USING stg_silver s\n",
    "               ON t.finding_id = s.finding_id\n",
    "            WHEN MATCHED AND (\n",
    "                   t.finding_modified_time IS NULL\n",
    "                OR s.finding_modified_time > t.finding_modified_time\n",
    "            ) THEN UPDATE SET\n",
    "                t.cf_processed_time     = s.cf_processed_time,\n",
    "                t.finding_modified_time = s.finding_modified_time,\n",
    "                t.finding_status        = s.finding_status,\n",
    "                t.account_id            = s.account_id,\n",
    "                t.region_id             = s.region_id,\n",
    "                t.standard_id           = s.standard_id,\n",
    "                t.control_id            = s.control_id,\n",
    "                t.compliance_status     = s.compliance_status,\n",
    "                t.severity              = s.severity\n",
    "            WHEN NOT MATCHED THEN INSERT (\n",
    "                finding_id, cf_processed_time, finding_modified_time,\n",
    "                finding_status, account_id, region_id, standard_id,\n",
    "                control_id, compliance_status, severity\n",
    "            ) VALUES (\n",
    "                s.finding_id, s.cf_processed_time, s.finding_modified_time,\n",
    "                s.finding_status, s.account_id, s.region_id, s.standard_id,\n",
    "                s.control_id, s.compliance_status, s.severity\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"✓ Silver MERGE completed\")\n",
    "\n",
    "        # ============================================================\n",
    "        # SILVER → GOLD: Aggregation\n",
    "        # ============================================================\n",
    "\n",
    "        # Load silver data\n",
    "        silver = (\n",
    "            spark.table(silver_tbl)\n",
    "            .where(F.col(\"cf_processed_time\") == F.to_timestamp(job_date))\n",
    "            .withColumn(\"compliance_status\", F.upper(\"compliance_status\"))\n",
    "            .withColumn(\n",
    "                \"severity\",\n",
    "                F.when(F.col(\"severity\").isNull(), \"unclassified\")\n",
    "                 .otherwise(F.lower(\"severity\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "        silver_count = silver.count()\n",
    "        print(f\"Silver rows for aggregation: {silver_count}\")\n",
    "\n",
    "        if silver_count == 0:\n",
    "            print(f\"⚠️  No silver data for {company_id}. Skipping gold aggregation...\")\n",
    "            skipped_companies.append((company_id, \"No silver data for job date\"))\n",
    "            continue\n",
    "\n",
    "        # Control-level aggregation\n",
    "        control_key = [\"cf_processed_time\", \"account_id\", \"region_id\", \"standard_id\", \"control_id\"]\n",
    "\n",
    "        controls = (\n",
    "            silver\n",
    "            .groupBy(*control_key)\n",
    "            .agg(\n",
    "                F.max(F.when(F.col(\"compliance_status\") == \"FAILED\", 1).otherwise(0)).alias(\"has_failed\"),\n",
    "                F.min(F.when(F.col(\"compliance_status\") == \"PASSED\", 1).otherwise(0)).alias(\"all_passed\"),\n",
    "                F.max(F.when(F.col(\"compliance_status\").isin(\"WARNING\", \"NOT_AVAILABLE\"), 1).otherwise(0)).alias(\"has_unknown\"),\n",
    "                F.max(\"severity\").alias(\"severity\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"control_status\",\n",
    "                F.when(F.col(\"has_failed\") == 1, \"FAILED\")\n",
    "                 .when(F.col(\"all_passed\") == 1, \"PASSED\")\n",
    "                 .otherwise(\"UNKNOWN\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Severity-level aggregation\n",
    "        std_key = [\"cf_processed_time\", \"account_id\", \"region_id\", \"standard_id\"]\n",
    "\n",
    "        severity_agg = (\n",
    "            controls\n",
    "            .groupBy(*std_key, \"severity\")\n",
    "            .agg(\n",
    "                F.countDistinct(\"control_id\").alias(\"total\"),\n",
    "                F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"passed\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"score\",\n",
    "                F.round(\n",
    "                    F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                     .otherwise(0.0),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Standard-level aggregation\n",
    "        standards = (\n",
    "            severity_agg\n",
    "            .groupBy(*std_key)\n",
    "            .agg(\n",
    "                F.sum(\"total\").alias(\"total\"),\n",
    "                F.sum(\"passed\").alias(\"passed\"),\n",
    "                F.collect_list(\n",
    "                    F.struct(\n",
    "                        F.col(\"severity\").alias(\"level\"),\n",
    "                        \"score\",\n",
    "                        F.struct(\"total\", \"passed\").alias(\"controls\")\n",
    "                    )\n",
    "                ).alias(\"controls_by_severity\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"score\",\n",
    "                F.round(\n",
    "                    F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                     .otherwise(0.0),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "            .select(\n",
    "                *std_key,\n",
    "                F.struct(\n",
    "                    F.col(\"standard_id\").alias(\"std\"),\n",
    "                    \"score\",\n",
    "                    F.struct(\"total\", \"passed\").alias(\"controls\"),\n",
    "                    \"controls_by_severity\"\n",
    "                ).alias(\"standard_summary\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Account/region summary\n",
    "        gold_key = [\"cf_processed_time\", \"account_id\", \"region_id\"]\n",
    "\n",
    "        overall = (\n",
    "            controls\n",
    "            .groupBy(*gold_key)\n",
    "            .agg(\n",
    "                F.countDistinct(F.struct(\"standard_id\", \"control_id\")).alias(\"total_rules\"),\n",
    "                F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"total_passed\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"control_pass_rate\",\n",
    "                F.when(F.col(\"total_rules\") > 0, F.col(\"total_passed\") / F.col(\"total_rules\"))\n",
    "                 .otherwise(0.0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        gold = (\n",
    "            overall\n",
    "            .join(\n",
    "                standards.groupBy(*gold_key)\n",
    "                         .agg(F.collect_list(\"standard_summary\").alias(\"standards_summary\")),\n",
    "                gold_key\n",
    "            )\n",
    "        )\n",
    "\n",
    "        gold_count = gold.count()\n",
    "        print(f\"Gold summary rows: {gold_count}\")\n",
    "\n",
    "        if gold_count > 0:\n",
    "            gold.createOrReplaceTempView(\"gold_updates\")\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {gold_tbl} t\n",
    "                USING gold_updates s\n",
    "                   ON t.cf_processed_time = s.cf_processed_time\n",
    "                  AND t.account_id = s.account_id\n",
    "                  AND t.region_id = s.region_id\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\")\n",
    "\n",
    "            print(\"✓ Gold MERGE completed\")\n",
    "\n",
    "        print(f\"✓ Successfully processed {company_id}\")\n",
    "        successful_companies.append(company_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR processing {company_id}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        failed_companies.append((company_id, str(e)))\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ETL Pipeline Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total companies: {len(companies_to_process)}\")\n",
    "print(f\"Successful: {len(successful_companies)}\")\n",
    "print(f\"Skipped: {len(skipped_companies)}\")\n",
    "print(f\"Failed: {len(failed_companies)}\")\n",
    "\n",
    "if successful_companies:\n",
    "    print(f\"\\n✓ Successful: {', '.join(successful_companies)}\")\n",
    "\n",
    "if skipped_companies:\n",
    "    print(f\"\\n⚠️  Skipped:\")\n",
    "    for comp_id, reason in skipped_companies:\n",
    "        print(f\"  - {comp_id}: {reason}\")\n",
    "\n",
    "if failed_companies:\n",
    "    print(f\"\\n❌ Failed:\")\n",
    "    for comp_id, error in failed_companies:\n",
    "        print(f\"  - {comp_id}: {error[:100]}...\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
