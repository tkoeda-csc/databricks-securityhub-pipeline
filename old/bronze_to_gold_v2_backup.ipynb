{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08279e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"CATALOG_NAME\", \"cloudfastener\")\n",
    "dbutils.widgets.text(\"COMPANY_INDEX_ID\", \"\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"CATALOG_NAME\").strip()\n",
    "company_index_id_param = dbutils.widgets.get(\"COMPANY_INDEX_ID\").strip()\n",
    "\n",
    "if not catalog_name:\n",
    "    raise ValueError(\"Missing required param: catalog_name\")\n",
    "\n",
    "# Set timezone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# Calculate job date and processing window\n",
    "# job_date = current day at 00:00:00 UTC\n",
    "# window = [job_date - 48 hours, job_date]\n",
    "# 48-hour window ensures we capture all findings (Security Hub checks every 18 hours)\n",
    "job_date = F.date_trunc(\"DAY\", F.current_timestamp())\n",
    "window_end_ts = F.to_timestamp(job_date)\n",
    "window_start_ts = window_end_ts - F.expr(\"INTERVAL 48 HOURS\")\n",
    "\n",
    "# cf_processed_time for data records (same as job_date)\n",
    "cf_processed_time = job_date\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BRONZE → GOLD ETL Pipeline (Direct, No Silver)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"catalog_name     = {catalog_name}\")\n",
    "print(f\"job_date         = {job_date}\")\n",
    "print(f\"window_start     = window_end - 48 hours\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400e19d",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae98a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_company_id(schema_name: str) -> bool:\n",
    "    \"\"\"Check if schema name matches company ID format: 12 chars, lowercase alphanumeric.\"\"\"\n",
    "    return (\n",
    "        len(schema_name) == 12 and\n",
    "        schema_name.isalnum() and\n",
    "        schema_name.islower()\n",
    "    )\n",
    "\n",
    "def discover_companies(catalog: str) -> list:\n",
    "    \"\"\"Discover all company schemas in the catalog.\"\"\"\n",
    "    try:\n",
    "        databases = spark.catalog.listDatabases()\n",
    "        companies = []\n",
    "        for db in databases:\n",
    "            # Schema format: catalog.company_id or just company_id\n",
    "            # Extract the last part after splitting by '.'\n",
    "            parts = db.name.split('.')\n",
    "            # Handle both formats: \"cloudfastener.xs22xw4aw73q\" or \"xs22xw4aw73q\"\n",
    "            if len(parts) >= 2 and parts[0] == catalog:\n",
    "                schema_name = parts[1]\n",
    "            elif len(parts) == 1:\n",
    "                schema_name = parts[0]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if is_valid_company_id(schema_name):\n",
    "                companies.append(schema_name)\n",
    "        return sorted(companies)\n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering companies: {e}\")\n",
    "        return []\n",
    "\n",
    "def table_exists(full_name: str) -> bool:\n",
    "    \"\"\"Check if a table exists in the catalog.\"\"\"\n",
    "    try:\n",
    "        return spark.catalog.tableExists(full_name)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalize_finding_id(col):\n",
    "    \"\"\"Normalize finding ID: trim and convert empty to NULL.\"\"\"\n",
    "    return F.when(F.length(F.trim(col)) == 0, F.lit(None)).otherwise(F.trim(col))\n",
    "\n",
    "def parse_iso8601_to_ts(col):\n",
    "    \"\"\"Parse ISO8601 timestamp string to Spark timestamp.\"\"\"\n",
    "    return F.to_timestamp(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e047eb6",
   "metadata": {},
   "source": [
    "## Discover Companies to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine companies to process\n",
    "if not company_index_id_param or company_index_id_param.upper() == \"ALL\":\n",
    "    companies_to_process = discover_companies(catalog_name)\n",
    "    print(f\"Auto-discovery mode: Found {len(companies_to_process)} companies\")\n",
    "    if companies_to_process:\n",
    "        print(f\"Companies: {', '.join(companies_to_process)}\")\n",
    "else:\n",
    "    # Single company mode\n",
    "    if not is_valid_company_id(company_index_id_param):\n",
    "        raise ValueError(f\"Invalid company_id format: {company_index_id_param}. Must be 12 lowercase alphanumeric characters.\")\n",
    "    companies_to_process = [company_index_id_param]\n",
    "    print(f\"Single company mode: {company_index_id_param}\")\n",
    "\n",
    "if not companies_to_process:\n",
    "    raise ValueError(\"No companies to process. Check catalog and schema names.\")\n",
    "\n",
    "print(f\"\\nTotal companies to process: {len(companies_to_process)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c3492",
   "metadata": {},
   "source": [
    "## Process Each Company\n",
    "\n",
    "Loop through each company and run the bronze → gold pipeline (no silver persistence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track results\n",
    "successful_companies = []\n",
    "failed_companies = []\n",
    "skipped_companies = []\n",
    "\n",
    "for company_id in companies_to_process:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Processing company: {company_id}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # Define table names for this company\n",
    "        asff_tbl = f\"{catalog_name}.{company_id}.aws_securityhub_findings_1_0\"\n",
    "        ocsf_tbl = f\"{catalog_name}.{company_id}.aws_securitylake_sh_findings_2_0\"\n",
    "        gold_tbl = f\"{catalog_name}.{company_id}.aws_standard_summary\"\n",
    "\n",
    "        print(f\"ASFF bronze      = {asff_tbl}\")\n",
    "        print(f\"OCSF bronze      = {ocsf_tbl}\")\n",
    "        print(f\"Gold             = {gold_tbl}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # ============================================================\n",
    "        # BRONZE → IN-MEMORY: Load and Transform\n",
    "        # ============================================================\n",
    "\n",
    "        # Check table existence\n",
    "        asff_exists = table_exists(asff_tbl)\n",
    "        ocsf_exists = table_exists(ocsf_tbl)\n",
    "\n",
    "        print(f\"ASFF exists? {asff_exists}\")\n",
    "        print(f\"OCSF exists? {ocsf_exists}\")\n",
    "\n",
    "        if not asff_exists and not ocsf_exists:\n",
    "            print(f\"⚠️  Neither bronze table exists for {company_id}. Skipping...\")\n",
    "            skipped_companies.append((company_id, \"No bronze tables\"))\n",
    "            continue\n",
    "\n",
    "        sources = []\n",
    "\n",
    "        # Load ASFF data\n",
    "        if asff_exists:\n",
    "            df_asff_raw = (\n",
    "                spark.table(asff_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"product_name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            asff_count = df_asff_raw.count()\n",
    "            print(f\"ASFF rows in window: {asff_count}\")\n",
    "            if asff_count > 0:\n",
    "                sources.append((\"ASFF\", df_asff_raw))\n",
    "\n",
    "        # Load OCSF data\n",
    "        if ocsf_exists:\n",
    "            df_ocsf_raw = (\n",
    "                spark.table(ocsf_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"metadata.product.name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            ocsf_count = df_ocsf_raw.count()\n",
    "            print(f\"OCSF rows in window: {ocsf_count}\")\n",
    "            if ocsf_count > 0:\n",
    "                sources.append((\"OCSF\", df_ocsf_raw))\n",
    "\n",
    "        if len(sources) == 0:\n",
    "            print(f\"⚠️  No rows found in window for {company_id}. Skipping...\")\n",
    "            skipped_companies.append((company_id, \"No data in window\"))\n",
    "            continue\n",
    "\n",
    "        # Transform Functions\n",
    "        def transform_asff(df):\n",
    "            return (\n",
    "                df\n",
    "                .where(F.col(\"RecordState\") != \"ARCHIVED\")  # Exclude archived findings\n",
    "                .select(\n",
    "                    normalize_finding_id(F.col(\"finding_id\")).alias(\"finding_id\"),\n",
    "                    parse_iso8601_to_ts(F.col(\"updated_at\")).alias(\"finding_modified_time\"),\n",
    "                    F.when(F.upper(F.col(\"workflow.Status\")) == \"NEW\", \"New\")\n",
    "                     .when(F.upper(F.col(\"workflow.Status\")) == \"NOTIFIED\", \"In Progress\")\n",
    "                     .when(F.upper(F.col(\"workflow.Status\")) == \"SUPPRESSED\", \"Suppressed\")\n",
    "                     .when(F.upper(F.col(\"workflow.Status\")) == \"RESOLVED\", \"Resolved\")\n",
    "                     .otherwise(F.col(\"workflow.Status\"))\n",
    "                     .alias(\"finding_status\"),\n",
    "                    F.col(\"aws_account_id\").cast(\"string\").alias(\"account_id\"),\n",
    "                    F.col(\"finding_region\").cast(\"string\").alias(\"region_id\"),\n",
    "                    F.expr(\"compliance.AssociatedStandards[0].StandardsId\").cast(\"string\").alias(\"standard_id\"),\n",
    "                    F.col(\"compliance.SecurityControlId\").cast(\"string\").alias(\"control_id\"),\n",
    "                    F.col(\"compliance.Status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "                    F.col(\"severity.Label\").cast(\"string\").alias(\"severity\"),\n",
    "                    F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "                    F.lit(1).alias(\"_preference\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        def transform_ocsf(df):\n",
    "            return (\n",
    "                df\n",
    "                .where(F.col(\"unmapped.RecordState\") != \"ARCHIVED\")  # Exclude archived findings\n",
    "                .select(\n",
    "                    normalize_finding_id(F.col(\"finding_info.uid\")).alias(\"finding_id\"),\n",
    "                    parse_iso8601_to_ts(F.col(\"finding_info.modified_time_dt\")).alias(\"finding_modified_time\"),\n",
    "                    F.when(F.upper(F.col(\"unmapped.WorkflowState\")) == \"SUPPRESSED\", \"Suppressed\")\n",
    "                     .otherwise(F.col(\"status\").cast(\"string\"))\n",
    "                     .alias(\"finding_status\"),\n",
    "                    F.col(\"cloud.account.uid\").cast(\"string\").alias(\"account_id\"),\n",
    "                    F.col(\"cloud.region\").cast(\"string\").alias(\"region_id\"),\n",
    "                    F.expr(\"compliance.standards[0]\").cast(\"string\").alias(\"standard_id\"),\n",
    "                    F.col(\"compliance.control\").cast(\"string\").alias(\"control_id\"),\n",
    "                    F.col(\"compliance.status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "                    F.col(\"severity\").cast(\"string\").alias(\"severity\"),\n",
    "                    F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "                    F.lit(0).alias(\"_preference\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Transform and union\n",
    "        canonical_dfs = []\n",
    "        for src, df_raw in sources:\n",
    "            if src == \"ASFF\":\n",
    "                out = transform_asff(df_raw)\n",
    "            elif src == \"OCSF\":\n",
    "                out = transform_ocsf(df_raw)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            out = out.withColumn(\"finding_id\", normalize_finding_id(F.col(\"finding_id\"))) \\\n",
    "                     .where(F.col(\"finding_id\").isNotNull())\n",
    "            canonical_dfs.append(out)\n",
    "\n",
    "        if not canonical_dfs:\n",
    "            print(f\"⚠️  No valid findings after filtering for {company_id}. Skipping...\")\n",
    "            skipped_companies.append((company_id, \"No valid findings\"))\n",
    "            continue\n",
    "\n",
    "        df_union = canonical_dfs[0]\n",
    "        for d in canonical_dfs[1:]:\n",
    "            df_union = df_union.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "        print(f\"Union rows: {df_union.count()}\")\n",
    "\n",
    "        # Deduplicate\n",
    "        w = Window.partitionBy(\"finding_id\").orderBy(\n",
    "            F.col(\"finding_modified_time\").desc_nulls_last(),\n",
    "            F.col(\"_preference\").desc(),\n",
    "            F.col(\"_bronze_processed_time\").desc_nulls_last()\n",
    "        )\n",
    "\n",
    "        findings = (\n",
    "            df_union\n",
    "            .withColumn(\"_rn\", F.row_number().over(w))\n",
    "            .where(F.col(\"_rn\") == 1)\n",
    "            .drop(\"_rn\", \"_preference\", \"_bronze_processed_time\")\n",
    "        )\n",
    "\n",
    "        findings_count = findings.count()\n",
    "        print(f\"Deduplicated findings: {findings_count} rows\")\n",
    "\n",
    "        # ============================================================\n",
    "        # IN-MEMORY → GOLD: Direct Aggregation\n",
    "        # ============================================================\n",
    "\n",
    "        # Normalize compliance status and severity\n",
    "        findings = (\n",
    "            findings\n",
    "            .withColumn(\"compliance_status\", F.upper(\"compliance_status\"))\n",
    "            .withColumn(\n",
    "                \"severity\",\n",
    "                F.when(F.col(\"severity\").isNull(), \"unclassified\")\n",
    "                 .otherwise(F.lower(\"severity\"))\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"is_suppressed\",\n",
    "                F.upper(F.col(\"finding_status\")) == F.lit(\"SUPPRESSED\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"severity_rank\",\n",
    "                F.when(F.col(\"severity\") == \"critical\", 4)\n",
    "                 .when(F.col(\"severity\") == \"high\", 3)\n",
    "                 .when(F.col(\"severity\") == \"medium\", 2)\n",
    "                 .when(F.col(\"severity\") == \"low\", 1)\n",
    "                 .otherwise(0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        active_count = findings.where(~F.col(\"is_suppressed\")).count()\n",
    "        suppressed_count = findings.where(F.col(\"is_suppressed\")).count()\n",
    "        print(f\"Active findings (non-suppressed): {active_count}\")\n",
    "        print(f\"Suppressed findings: {suppressed_count}\")\n",
    "\n",
    "        # Control-level aggregation\n",
    "        control_key = [\"account_id\", \"region_id\", \"standard_id\", \"control_id\"]\n",
    "\n",
    "        controls = (\n",
    "            findings\n",
    "            .groupBy(*control_key)\n",
    "            .agg(\n",
    "                F.sum(F.when(~F.col(\"is_suppressed\"), 1).otherwise(0)).alias(\"active_cnt\"),\n",
    "                F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\") == \"FAILED\"), 1).otherwise(0)).alias(\"failed_cnt\"),\n",
    "                F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\") == \"PASSED\"), 1).otherwise(0)).alias(\"passed_cnt\"),\n",
    "                F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\").isin(\"WARNING\", \"NOT_AVAILABLE\")), 1).otherwise(0)).alias(\"unknown_cnt\"),\n",
    "                F.count(\"*\").alias(\"total_cnt\"),\n",
    "                F.max(\"severity_rank\").alias(\"max_severity_rank\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"control_status\",\n",
    "                F.when(F.col(\"active_cnt\") == 0, \"NO_DATA\")\n",
    "                 .when(F.col(\"failed_cnt\") > 0, \"FAILED\")\n",
    "                 .when(F.col(\"unknown_cnt\") > 0, \"UNKNOWN\")\n",
    "                 .when(F.col(\"passed_cnt\") == F.col(\"active_cnt\"), \"PASSED\")\n",
    "                 .otherwise(\"UNKNOWN\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"severity\",\n",
    "                F.when(F.col(\"max_severity_rank\") == 4, \"critical\")\n",
    "                 .when(F.col(\"max_severity_rank\") == 3, \"high\")\n",
    "                 .when(F.col(\"max_severity_rank\") == 2, \"medium\")\n",
    "                 .when(F.col(\"max_severity_rank\") == 1, \"low\")\n",
    "                 .otherwise(\"unclassified\")\n",
    "            )\n",
    "            .drop(\"max_severity_rank\")\n",
    "        )\n",
    "\n",
    "        # Severity-level aggregation\n",
    "        std_key = [\"account_id\", \"region_id\", \"standard_id\"]\n",
    "\n",
    "        severity_agg = (\n",
    "            controls\n",
    "            .groupBy(*std_key, \"severity\")\n",
    "            .agg(\n",
    "                F.countDistinct(\"control_id\").alias(\"total\"),\n",
    "                F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"passed\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"score\",\n",
    "                F.round(\n",
    "                    F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                     .otherwise(0.0),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Standard-level aggregation\n",
    "        standards = (\n",
    "            severity_agg\n",
    "            .groupBy(*std_key)\n",
    "            .agg(\n",
    "                F.sum(\"total\").alias(\"total\"),\n",
    "                F.sum(\"passed\").alias(\"passed\"),\n",
    "                F.collect_list(\n",
    "                    F.struct(\n",
    "                        F.col(\"severity\").alias(\"level\"),\n",
    "                        \"score\",\n",
    "                        F.struct(\"total\", \"passed\").alias(\"controls\")\n",
    "                    )\n",
    "                ).alias(\"controls_by_severity\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"score\",\n",
    "                F.round(\n",
    "                    F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                     .otherwise(0.0),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "            .select(\n",
    "                *std_key,\n",
    "                F.struct(\n",
    "                    F.col(\"standard_id\").alias(\"std\"),\n",
    "                    \"score\",\n",
    "                    F.struct(\"total\", \"passed\").alias(\"controls\"),\n",
    "                    \"controls_by_severity\"\n",
    "                ).alias(\"standard_summary\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Account/region summary\n",
    "        gold_key = [\"account_id\", \"region_id\"]\n",
    "\n",
    "        overall = (\n",
    "            controls\n",
    "            .groupBy(*gold_key)\n",
    "            .agg(\n",
    "                F.countDistinct(F.struct(\"standard_id\", \"control_id\")).alias(\"total_rules\"),\n",
    "                F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"total_passed\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"control_pass_score\",\n",
    "                F.round(\n",
    "                    F.when(F.col(\"total_rules\") > 0, (F.col(\"total_passed\") / F.col(\"total_rules\")) * 100)\n",
    "                     .otherwise(0.0),\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        gold = (\n",
    "            overall\n",
    "            .join(\n",
    "                standards.groupBy(*gold_key)\n",
    "                         .agg(F.collect_list(\"standard_summary\").alias(\"standards_summary\")),\n",
    "                gold_key\n",
    "            )\n",
    "            .withColumn(\"cf_processed_time\", F.to_timestamp(F.lit(cf_processed_time)))\n",
    "            .withColumn(\"company_id\", F.lit(company_id))\n",
    "            .select(\n",
    "                \"company_id\",\n",
    "                \"cf_processed_time\",\n",
    "                \"account_id\",\n",
    "                \"region_id\",\n",
    "                \"control_pass_score\",\n",
    "                \"total_rules\",\n",
    "                \"total_passed\",\n",
    "                \"standards_summary\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        gold_count = gold.count()\n",
    "        print(f\"Gold summary rows: {gold_count}\")\n",
    "\n",
    "        if gold_count > 0:\n",
    "            # Create gold table if it doesn't exist\n",
    "            spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {gold_tbl} (\n",
    "              company_id STRING,\n",
    "              cf_processed_time TIMESTAMP,\n",
    "              account_id STRING,\n",
    "              region_id STRING,\n",
    "              control_pass_score FLOAT,\n",
    "              total_rules INT,\n",
    "              total_passed INT,\n",
    "              standards_summary ARRAY<STRUCT<\n",
    "                std: STRING,\n",
    "                score: FLOAT,\n",
    "                controls: STRUCT<\n",
    "                  total: INT,\n",
    "                  passed: INT\n",
    "                >,\n",
    "                controls_by_severity: ARRAY<STRUCT<\n",
    "                  level: STRING,\n",
    "                  score: FLOAT,\n",
    "                  controls: STRUCT<\n",
    "                    total: INT,\n",
    "                    passed: INT\n",
    "                  >\n",
    "                >>\n",
    "              >>\n",
    "            )\n",
    "            USING DELTA\n",
    "            \"\"\")\n",
    "\n",
    "            # Clear old data (1-day retention strategy)\n",
    "            spark.sql(f\"TRUNCATE TABLE {gold_tbl}\")\n",
    "\n",
    "            # Write new data\n",
    "            gold.write.mode(\"append\").insertInto(gold_tbl)\n",
    "\n",
    "            print(\"✓ Gold table overwritten (1-day retention)\")\n",
    "\n",
    "        print(f\"✓ Successfully processed {company_id}\")\n",
    "        successful_companies.append(company_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR processing {company_id}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        failed_companies.append((company_id, str(e)))\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BRONZE → GOLD ETL PIPELINE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total companies:       {len(companies_to_process)}\")\n",
    "print(f\"✓ Successful:          {len(successful_companies)}\")\n",
    "print(f\"⚠️  Skipped:            {len(skipped_companies)}\")\n",
    "print(f\"❌ Failed:             {len(failed_companies)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if successful_companies:\n",
    "    print(f\"\\n✓ SUCCESSFUL ({len(successful_companies)}):\")\n",
    "    for comp_id in successful_companies:\n",
    "        print(f\"  - {comp_id}\")\n",
    "\n",
    "if skipped_companies:\n",
    "    print(f\"\\n⚠️  SKIPPED ({len(skipped_companies)}):\")\n",
    "    for comp_id, reason in skipped_companies:\n",
    "        print(f\"  - [{comp_id}] {reason}\")\n",
    "\n",
    "if failed_companies:\n",
    "    print(f\"\\n❌ FAILED ({len(failed_companies)}):\")\n",
    "    for comp_id, error in failed_companies:\n",
    "        print(f\"  - [{comp_id}] {error[:150]}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    raise Exception(f\"Bronze→Gold processing failed for {len(failed_companies)} company(ies). See details above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Bronze→Gold ETL completed successfully ✓\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
