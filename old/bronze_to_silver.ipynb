{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905408f3-65fb-4641-86b1-5471ce2ad767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1566b03c-b788-467f-9fc9-13bf18b74402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nBronze → Silver job starting\ncompany_id   = xs22xw4aw73q\nsnapshot_at  = Column<'date_trunc(DAY, to_timestamp(2025-12-17T00:00:00Z))'> (UTC logical boundary)\nwindow_start = snapshot_at - 1 day\nASFF bronze  = cloudfastener.xs22xw4aw73q.aws_securityhub_findings_1_0\nOCSF bronze  = cloudfastener.xs22xw4aw73q.aws_securitylake_sh_findings_2_0\nSilver       = cloudfastener.xs22xw4aw73q.silver_aws_compliance_findings\n============================================================\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(\"company_id\", \"\")\n",
    "dbutils.widgets.text(\"snapshot_at\", \"\") \n",
    "\n",
    "company_id  = dbutils.widgets.get(\"company_id\").strip()\n",
    "snapshot_at = dbutils.widgets.get(\"snapshot_at\").strip()\n",
    " \n",
    "if not company_id:\n",
    "    raise ValueError(\"Missing required param: company_id\")\n",
    "if not snapshot_at:\n",
    "    raise ValueError(\"Missing required param: snapshot_at (UTC boundary like 2025-12-17T00:00:00Z)\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "asff_tbl   = f\"cloudfastener.{company_id}.aws_securityhub_findings_1_0\"\n",
    "ocsf_tbl   = f\"cloudfastener.{company_id}.aws_securitylake_sh_findings_2_0\"\n",
    "silver_tbl = f\"cloudfastener.{company_id}.silver_aws_compliance_findings\"\n",
    "\n",
    "snapshot_at = F.date_trunc(\n",
    "    \"DAY\",\n",
    "    F.to_timestamp(F.lit(snapshot_at)))\n",
    "\n",
    "window_end_ts   = F.to_timestamp(F.lit(snapshot_at))\n",
    "window_start_ts = snapshot_at - F.expr(\"INTERVAL 1 DAY\")\n",
    "\n",
    "print(\"============================================================\")\n",
    "print(\"Bronze → Silver job starting\")\n",
    "print(f\"company_id   = {company_id}\")\n",
    "print(f\"snapshot_at  = {snapshot_at} (UTC logical boundary)\")\n",
    "print(f\"window_start = snapshot_at - 1 day\")\n",
    "print(f\"ASFF bronze  = {asff_tbl}\")\n",
    "print(f\"OCSF bronze  = {ocsf_tbl}\")\n",
    "print(f\"Silver       = {silver_tbl}\")\n",
    "print(\"============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02fdb85-e769-47a5-afa5-90590c4c6719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists(full_name: str) -> bool:\n",
    "    try:\n",
    "        return spark.catalog.tableExists(full_name)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalize_finding_id(col):\n",
    "    # trim, convert empty to NULL\n",
    "    return F.when(F.length(F.trim(col)) == 0, F.lit(None)).otherwise(F.trim(col))\n",
    "\n",
    "def parse_iso8601_to_ts(col):\n",
    "    # Works for ISO8601 strings like \"...Z\"\n",
    "    return F.to_timestamp(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3929fa65-8138-4640-9fb7-dff3309c0bbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASFF exists? True\nOCSF exists? True\nASFF rows in window: 0\nOCSF rows in window: 65316\n"
     ]
    }
   ],
   "source": [
    "asff_exists = table_exists(asff_tbl)\n",
    "ocsf_exists = table_exists(ocsf_tbl)\n",
    "\n",
    "print(f\"ASFF exists? {asff_exists}\")\n",
    "print(f\"OCSF exists? {ocsf_exists}\")\n",
    "\n",
    "if not asff_exists and not ocsf_exists:\n",
    "    raise RuntimeError(\"Neither bronze table exists; cannot build silver for this company.\")\n",
    "\n",
    "sources = []\n",
    "\n",
    "if asff_exists:\n",
    "    df_asff_raw = (\n",
    "        spark.table(asff_tbl)\n",
    "        .where(\n",
    "            (F.col(\"product_name\") == \"Security Hub\") &(F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "            (F.col(\"cf_processed_time\") <  window_end_ts))\n",
    "    )\n",
    "    asff_count = df_asff_raw.count()\n",
    "    print(f\"ASFF rows in window: {asff_count}\")\n",
    "    if asff_count > 0:\n",
    "        sources.append((\"ASFF\", df_asff_raw))\n",
    "\n",
    "if ocsf_exists:\n",
    "    df_ocsf_raw = (\n",
    "        spark.table(ocsf_tbl)\n",
    "        .where(\n",
    "            (F.col(\"metadata.product.name\") == \"Security Hub\") &\n",
    "            (F.col(\"cf_processed_time\") >= window_start_ts) \n",
    "            &\n",
    "               (F.col(\"cf_processed_time\") <  window_end_ts))\n",
    "    )\n",
    "    ocsf_count = df_ocsf_raw.count()\n",
    "    print(f\"OCSF rows in window: {ocsf_count}\")\n",
    "    if ocsf_count > 0:\n",
    "        sources.append((\"OCSF\", df_ocsf_raw))\n",
    "\n",
    "if len(sources) == 0:\n",
    "    print(\"No rows found in the window for existing sources. Nothing to merge.\")\n",
    "    dbutils.notebook.exit(\"EMPTY_WINDOW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18426c53-431e-45d3-b5ed-0798f27dfade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def transform_asff(df):\n",
    "    return (\n",
    "        df.select(\n",
    "            # identity\n",
    "            F.lit(company_id).alias(\"company_id\"),\n",
    "            F.lit(\"ASFF\").alias(\"finding_source\"),\n",
    "\n",
    "            # key\n",
    "            normalize_finding_id(F.col(\"finding_id\")).alias(\"finding_id\"),\n",
    "\n",
    "            # times (ASFF columns are STRING in your schema dump)\n",
    "            parse_iso8601_to_ts(F.col(\"created_at\")).alias(\"finding_created_time\"),\n",
    "            parse_iso8601_to_ts(F.col(\"updated_at\")).alias(\"finding_modified_time\"),\n",
    "\n",
    "            # status (normalize ASFF workflow statuses into the OCSF-style set)\n",
    "            F.when(F.upper(F.col(\"workflow.Status\")) == \"NEW\", \"New\")\n",
    "             .when(F.upper(F.col(\"workflow.Status\")) == \"NOTIFIED\", \"In Progress\")\n",
    "             .when(F.upper(F.col(\"workflow.Status\")) == \"SUPPRESSED\", \"Suppressed\")\n",
    "             .when(F.upper(F.col(\"workflow.Status\")) == \"RESOLVED\", \"Resolved\")\n",
    "             .otherwise(F.col(\"workflow.Status\"))\n",
    "             .alias(\"finding_status\"),\n",
    "\n",
    "            # account/region\n",
    "            F.col(\"aws_account_id\").cast(\"string\").alias(\"account_id\"),\n",
    "            F.col(\"finding_region\").cast(\"string\").alias(\"region_id\"),\n",
    "\n",
    "            # compliance / control mapping (correct for your ASFF schema)\n",
    "            F.expr(\"compliance.AssociatedStandards[0].StandardsId\").cast(\"string\").alias(\"standard_id\"),\n",
    "            F.col(\"compliance.SecurityControlId\").cast(\"string\").alias(\"control_id\"),\n",
    "            F.col(\"compliance.Status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "\n",
    "            # control title\n",
    "            F.col(\"title\").cast(\"string\").alias(\"control_title\"),\n",
    "\n",
    "            # severity (struct<Label,Normalized,Original>)\n",
    "            F.col(\"severity.Label\").cast(\"string\").alias(\"severity\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"rule_severity\"),\n",
    "\n",
    "            # resource (array<struct<Type,Id,...>>; note the capitalized keys)\n",
    "            F.expr(\"resources[0].Id\").cast(\"string\").alias(\"resource_id\"),\n",
    "            F.expr(\"resources[0].Type\").cast(\"string\").alias(\"resource_type\"),\n",
    "\n",
    "            # snapshot for this run\n",
    "            F.to_timestamp(F.lit(snapshot_at)).alias(\"snapshot_at\"),\n",
    "\n",
    "            # tie-break helpers for dedupe\n",
    "            F.col(\"cf_processed_time\").alias(\"_cf_processed_time\"),\n",
    "            F.lit(0).alias(\"_source_preference\")\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd1ee3a-eb24-41ac-b159-b774bd2d0e58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_ocsf(df):\n",
    "    return (\n",
    "        df.select(\n",
    "            F.lit(company_id).alias(\"company_id\"),\n",
    "            F.lit(\"OCSF\").alias(\"finding_source\"),\n",
    "\n",
    "            normalize_finding_id(F.col(\"finding_info.uid\")).alias(\"finding_id\"),\n",
    "\n",
    "            parse_iso8601_to_ts(F.col(\"finding_info.created_time_dt\")).alias(\"finding_created_time\"),\n",
    "            parse_iso8601_to_ts(F.col(\"finding_info.modified_time_dt\")).alias(\"finding_modified_time\"),\n",
    "\n",
    "            F.col(\"status\").cast(\"string\").alias(\"finding_status\"),\n",
    "\n",
    "            F.col(\"cloud.account.uid\").cast(\"string\").alias(\"account_id\"),\n",
    "            F.col(\"cloud.region\").cast(\"string\").alias(\"region_id\"),\n",
    "\n",
    "            F.expr(\"compliance.standards[0]\").cast(\"string\").alias(\"standard_id\"),\n",
    "            F.col(\"compliance.control\").cast(\"string\").alias(\"control_id\"),\n",
    "            F.col(\"compliance.status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "\n",
    "            F.col(\"finding_info.title\").cast(\"string\").alias(\"control_title\"),\n",
    "\n",
    "            F.col(\"severity\").cast(\"string\").alias(\"severity\"),\n",
    "            F.lit(None).cast(\"string\").alias(\"rule_severity\"),\n",
    "\n",
    "            F.coalesce(\n",
    "                F.expr(\"resources[0].uid\").cast(\"string\"),\n",
    "                F.col(\"resource.uid\").cast(\"string\"),\n",
    "                F.col(\"unmapped\").getItem(\"ProductFields.Resources:0/Id\").cast(\"string\")\n",
    "            ).alias(\"resource_id\"),\n",
    "\n",
    "            F.coalesce(\n",
    "                F.expr(\"resources[0].type\").cast(\"string\"),\n",
    "                F.col(\"resource.type\").cast(\"string\"),\n",
    "                F.col(\"unmapped\").getItem(\"ProductFields.Resources:0/Type\").cast(\"string\")\n",
    "            ).alias(\"resource_type\"),\n",
    "\n",
    "            F.to_timestamp(F.lit(snapshot_at)).alias(\"snapshot_at\"),\n",
    "\n",
    "            F.col(\"cf_processed_time\").alias(\"_cf_processed_time\"),\n",
    "            F.lit(1).alias(\"_source_preference\")\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "979ef964-0fcd-47c2-81f4-81729d5c432b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union rows: 65316\n"
     ]
    }
   ],
   "source": [
    "canonical_dfs = []\n",
    "\n",
    "for src, dfraw in sources:\n",
    "    if src == \"ASFF\":\n",
    "        out = transform_asff(dfraw)\n",
    "    elif src == \"OCSF\":\n",
    "        out = transform_ocsf(dfraw)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if out is None:\n",
    "        raise RuntimeError(f\"Transform returned None for source={src}. Check function definitions.\")\n",
    "\n",
    "    # Keep only valid keys early; it reduces later work\n",
    "    out = out.withColumn(\"finding_id\", normalize_finding_id(F.col(\"finding_id\"))) \\\n",
    "             .where(F.col(\"finding_id\").isNotNull())\n",
    "\n",
    "    canonical_dfs.append(out)\n",
    "\n",
    "if not canonical_dfs:\n",
    "    print(\"No canonical rows after filtering (empty window or null keys). Nothing to merge.\")\n",
    "    dbutils.notebook.exit(\"EMPTY_CANONICAL\")\n",
    "\n",
    "df_union = canonical_dfs[0]\n",
    "for d in canonical_dfs[1:]:\n",
    "    df_union = df_union.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "print(f\"Union rows: {df_union.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3493688-3b61-4254-9cd2-9c8ad7dd2f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winners rows: 4083\nDistinct finding_id in winners: 4083\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"company_id\", \"finding_id\").orderBy(\n",
    "    F.col(\"finding_modified_time\").desc_nulls_last(),\n",
    "    F.col(\"_source_preference\").desc(),\n",
    "    F.col(\"_cf_processed_time\").desc_nulls_last()\n",
    ")\n",
    "\n",
    "df_winners = (\n",
    "    df_union\n",
    "    .withColumn(\"_rn\", F.row_number().over(w))\n",
    "    .where(F.col(\"_rn\") == 1)\n",
    ")\n",
    "\n",
    "winner_count = df_winners.count()\n",
    "distinct_ids = df_winners.select(\"finding_id\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa34dbe1-3e2c-4f33-ac3a-07be85efe3b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE completed.\nJob done.\n"
     ]
    }
   ],
   "source": [
    "df_stage = (\n",
    "    df_winners\n",
    "    .withColumn(\"source_preference\", F.col(\"_source_preference\"))\n",
    "    .withColumn(\"cf_processed_time\", F.col(\"_cf_processed_time\"))\n",
    "    .drop(\"_rn\", \"_source_preference\", \"_cf_processed_time\")\n",
    ")\n",
    "\n",
    "df_stage.createOrReplaceTempView(\"stg_winners\")\n",
    "\n",
    "# source_preference: higher wins\n",
    "# ASFF = 0 (lower priority)\n",
    "# OCSF = 1 (higher priority)\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {silver_tbl} t\n",
    "    USING stg_winners s\n",
    "       ON t.company_id = s.company_id \n",
    "      AND t.finding_id = s.finding_id\n",
    "\n",
    "    WHEN MATCHED AND (\n",
    "           t.finding_modified_time IS NULL\n",
    "        OR s.finding_modified_time > t.finding_modified_time\n",
    "        OR (\n",
    "             s.finding_modified_time = t.finding_modified_time\n",
    "             AND (CASE WHEN t.finding_source = 'OCSF' THEN 1 ELSE 0 END) < s.source_preference\n",
    "           )\n",
    "    ) THEN UPDATE SET\n",
    "        t.finding_source        = s.finding_source,\n",
    "        t.finding_created_time  = s.finding_created_time,\n",
    "        t.finding_modified_time = s.finding_modified_time,\n",
    "        t.snapshot_at           = s.snapshot_at,\n",
    "        t.finding_status        = s.finding_status,\n",
    "        t.account_id            = s.account_id,\n",
    "        t.region_id             = s.region_id,\n",
    "        t.standard_id           = s.standard_id,\n",
    "        t.control_id            = s.control_id,\n",
    "        t.control_title         = s.control_title,\n",
    "        t.severity              = s.severity,\n",
    "        t.rule_severity         = s.rule_severity,\n",
    "        t.compliance_status     = s.compliance_status,\n",
    "        t.resource_id           = s.resource_id,\n",
    "        t.resource_type         = s.resource_type,\n",
    "        t.updated_at            = current_timestamp()\n",
    "\n",
    "    WHEN NOT MATCHED THEN INSERT (\n",
    "        company_id, \n",
    "        finding_source, \n",
    "        created_at, \n",
    "        updated_at, \n",
    "        finding_id,\n",
    "        finding_created_time, \n",
    "        finding_modified_time, \n",
    "        snapshot_at, \n",
    "        finding_status,\n",
    "        account_id, \n",
    "        region_id, \n",
    "        standard_id, \n",
    "        control_id, \n",
    "        control_title, \n",
    "        severity,\n",
    "        rule_severity, \n",
    "        compliance_status, \n",
    "        resource_id, \n",
    "        resource_type\n",
    "    ) VALUES (\n",
    "        s.company_id, \n",
    "        s.finding_source, \n",
    "        current_timestamp(), \n",
    "        current_timestamp(), \n",
    "        s.finding_id,\n",
    "        s.finding_created_time, \n",
    "        s.finding_modified_time, \n",
    "        s.snapshot_at, \n",
    "        s.finding_status,\n",
    "        s.account_id, \n",
    "        s.region_id, \n",
    "        s.standard_id, \n",
    "        s.control_id, \n",
    "        s.control_title, \n",
    "        s.severity,\n",
    "        s.rule_severity, \n",
    "        s.compliance_status, \n",
    "        s.resource_id, \n",
    "        s.resource_type\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"MERGE completed.\")\n",
    "print(\"Job done.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4957998520579432,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_silver",
   "widgets": {
    "company_id": {
     "currentValue": "xs22xw4aw73q",
     "nuid": "92e4a9f9-ffdd-4893-8611-da893c30ec49",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "company_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "company_id",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "snapshot_at": {
     "currentValue": "2025-12-17T00:00:00Z",
     "nuid": "ab71edaa-5b45-49f1-9d25-99bafcf4c589",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "snapshot_at",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "snapshot_at",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}