{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1836d6",
   "metadata": {},
   "source": [
    "# Security Hub Compliance Export Pipeline (JSONL Format)\n",
    "\n",
    "AWS Security Hub compliance data processing and S3 export using JSONL format for optimal performance and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892aea4",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n",
    "\n",
    "Load parameters, configure Spark, and define the processing time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08279e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# Get parameters from job\n",
    "dbutils.widgets.text(\"CATALOG_NAME\", \"\")\n",
    "dbutils.widgets.text(\"COMPANY_INDEX_ID\", \"\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"CATALOG_NAME\").strip()\n",
    "company_index_id_param = dbutils.widgets.get(\"COMPANY_INDEX_ID\").strip()\n",
    "\n",
    "if not catalog_name:\n",
    "    raise ValueError(\"Missing required param: CATALOG_NAME\")\n",
    "\n",
    "# Set timezone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# Calculate job date and processing window\n",
    "# 48-hour window ensures we capture all findings (Security Hub checks every 18 hours)\n",
    "job_date = F.date_trunc(\"DAY\", F.current_timestamp())\n",
    "window_end_ts = job_date\n",
    "window_start_ts = window_end_ts - F.expr(\"INTERVAL 48 HOURS\")\n",
    "\n",
    "# S3 output configuration\n",
    "# Structure: s3://bucket/company_id/cloud_type/data_type/YYYY-MM-DD/\n",
    "# Format: JSONL (JSON Lines) - one JSON object per line\n",
    "s3_base_path = \"s3://dev-cf-databricks-catalog-bucket/dev/dashboard\"\n",
    "date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Determine processing mode\n",
    "is_all_companies = not company_index_id_param or company_index_id_param.upper() == \"ALL\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECURITY HUB STANDARDS ETL PIPELINE (JSONL FORMAT)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Catalog:            {catalog_name}\")\n",
    "print(f\"Company Mode:       {'Auto-Discovery (ALL)' if is_all_companies else company_index_id_param}\")\n",
    "print(f\"Job Date:           {date_str}\")\n",
    "print(f\"Time Window:        48 hours (Security Hub check cycle: 18 hours)\")\n",
    "print(f\"Output Format:      JSONL (JSON Lines) with gzip compression\")\n",
    "print(f\"Output Structure:   company_id/aws/data_type/{date_str}/\")\n",
    "print(f\"S3 Base Path:       {s3_base_path}\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11531b6c",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for company ID validation, table existence checks, and company discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_company_id(schema_name: str) -> bool:\n",
    "    \"\"\"Check if schema name matches company ID format: 12 chars, lowercase alphanumeric.\"\"\"\n",
    "    return (\n",
    "        len(schema_name) == 12 and\n",
    "        schema_name.isalnum() and\n",
    "        schema_name.islower()\n",
    "    )\n",
    "\n",
    "def table_exists(full_name: str) -> bool:\n",
    "    \"\"\"Check if a table exists in the catalog.\"\"\"\n",
    "    try:\n",
    "        return spark.catalog.tableExists(full_name)\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae98a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_companies(catalog: str) -> list:\n",
    "    \"\"\"Discover all company schemas in the catalog.\"\"\"\n",
    "    try:\n",
    "        # Unity Catalog: need to list schemas within the catalog\n",
    "        print(f\"[DEBUG] Searching for companies in catalog: {catalog}\")\n",
    "\n",
    "        # Set current catalog and list schemas\n",
    "        spark.sql(f\"USE CATALOG {catalog}\")\n",
    "        schemas = spark.catalog.listDatabases()\n",
    "\n",
    "        print(f\"[DEBUG] Total schemas found in {catalog}: {len(schemas)}\")\n",
    "\n",
    "        companies = []\n",
    "        for schema in schemas:\n",
    "            schema_name = schema.name\n",
    "            print(f\"[DEBUG] Checking schema: {schema_name}\")\n",
    "\n",
    "            # In Unity Catalog, schema.name is just the schema name (not catalog.schema)\n",
    "            # But might still have catalog prefix in some cases\n",
    "            if '.' in schema_name:\n",
    "                # Handle \"catalog.schema\" format\n",
    "                parts = schema_name.split('.')\n",
    "                if parts[0] == catalog and len(parts) == 2:\n",
    "                    schema_name = parts[1]\n",
    "                    print(f\"[DEBUG]   -> Extracted schema: {schema_name}\")\n",
    "                else:\n",
    "                    print(f\"[DEBUG]   -> Skipped (unexpected format: {schema_name})\")\n",
    "                    continue\n",
    "\n",
    "            if is_valid_company_id(schema_name):\n",
    "                print(f\"[DEBUG]   -> ✓ Valid company ID: {schema_name}\")\n",
    "                companies.append(schema_name)\n",
    "            else:\n",
    "                print(f\"[DEBUG]   -> ✗ Invalid company ID format: {schema_name} (len={len(schema_name)}, alnum={schema_name.isalnum()}, lower={schema_name.islower()})\")\n",
    "\n",
    "        return sorted(companies)\n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering companies: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d255863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_finding_id(col):\n",
    "    \"\"\"Normalize finding ID: trim and convert empty to NULL.\"\"\"\n",
    "    return F.when(F.length(F.trim(col)) == 0, F.lit(None)).otherwise(F.trim(col))\n",
    "\n",
    "def parse_iso8601_to_ts(col):\n",
    "    \"\"\"Parse ISO8601 timestamp string to Spark timestamp.\"\"\"\n",
    "    return F.to_timestamp(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2064714a",
   "metadata": {},
   "source": [
    "## Load Security Hub Controls Reference\n",
    "\n",
    "Load the reference table mapping control IDs to correct severity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD SECURITY HUB CONTROLS REFERENCE TABLE\n",
    "# ============================================================\n",
    "\n",
    "# Load the reference table with correct control_id -> severity mappings\n",
    "controls_ref_table = f\"{catalog_name}.reference.securityhub_controls\"\n",
    "\n",
    "try:\n",
    "    if table_exists(controls_ref_table):\n",
    "        controls_ref_df = spark.table(controls_ref_table).select(\"control_id\", \"severity\")\n",
    "        ref_count = controls_ref_df.count()\n",
    "        print(f\"[INFO] Loaded Security Hub controls reference table: {ref_count} mappings\")\n",
    "    else:\n",
    "        print(f\"[WARN] Reference table {controls_ref_table} not found. Severity will be taken from source data.\")\n",
    "        controls_ref_df = None\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not load reference table: {e}. Severity will be taken from source data.\")\n",
    "    controls_ref_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20139ea",
   "metadata": {},
   "source": [
    "## Data Transformation Functions\n",
    "\n",
    "Transform ASFF and OCSF formats to canonical schema, excluding archived findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_asff(df):\n",
    "    \"\"\"\n",
    "    Transform ASFF (AWS Security Finding Format) to canonical schema.\n",
    "    Excludes ARCHIVED findings and normalizes fields.\n",
    "    Preserves original workflow status values: NEW, NOTIFIED, SUPPRESSED, RESOLVED.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .where(F.col(\"RecordState\") != \"ARCHIVED\")\n",
    "        .select(\n",
    "            normalize_finding_id(F.col(\"finding_id\")).alias(\"finding_id\"),\n",
    "            parse_iso8601_to_ts(F.col(\"updated_at\")).alias(\"finding_modified_time\"),\n",
    "            F.upper(F.col(\"workflow.Status\")).alias(\"finding_status\"),\n",
    "            F.col(\"aws_account_id\").cast(\"string\").alias(\"account_id\"),\n",
    "            F.col(\"finding_region\").cast(\"string\").alias(\"region_id\"),\n",
    "            F.expr(\"compliance.AssociatedStandards[0].StandardsId\").cast(\"string\").alias(\"standard_id\"),\n",
    "            F.col(\"compliance.SecurityControlId\").cast(\"string\").alias(\"control_id\"),\n",
    "            F.col(\"compliance.Status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "            F.col(\"severity.Label\").cast(\"string\").alias(\"severity\"),\n",
    "            F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "            F.lit(1).alias(\"_preference\")  # ASFF preferred over OCSF\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ocsf(df):\n",
    "    \"\"\"\n",
    "    Transform OCSF (Open Cybersecurity Schema Framework) to canonical schema.\n",
    "    Excludes ARCHIVED findings and normalizes fields.\n",
    "    Maps workflow status to match ASFF format: NEW, NOTIFIED, SUPPRESSED, RESOLVED.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .where(F.col(\"unmapped.RecordState\") != \"ARCHIVED\")\n",
    "        .select(\n",
    "            normalize_finding_id(F.col(\"finding_info.uid\")).alias(\"finding_id\"),\n",
    "            parse_iso8601_to_ts(F.col(\"finding_info.modified_time_dt\")).alias(\"finding_modified_time\"),\n",
    "            # Map OCSF workflow states to match ASFF uppercase format\n",
    "            F.when(F.col(\"unmapped.WorkflowState\").isNotNull(), F.upper(F.col(\"unmapped.WorkflowState\")))\n",
    "             .when(F.upper(F.col(\"status\").cast(\"string\")) == \"IN_PROGRESS\", \"NOTIFIED\")\n",
    "             .otherwise(F.upper(F.col(\"status\").cast(\"string\")))\n",
    "             .alias(\"finding_status\"),\n",
    "            F.col(\"cloud.account.uid\").cast(\"string\").alias(\"account_id\"),\n",
    "            F.col(\"cloud.region\").cast(\"string\").alias(\"region_id\"),\n",
    "            F.expr(\"compliance.standards[0]\").cast(\"string\").alias(\"standard_id\"),\n",
    "            F.col(\"compliance.control\").cast(\"string\").alias(\"control_id\"),\n",
    "            F.col(\"compliance.status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "            F.col(\"severity\").cast(\"string\").alias(\"severity\"),\n",
    "            F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "            F.lit(0).alias(\"_preference\")  # OCSF fallback\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76431ab",
   "metadata": {},
   "source": [
    "## Aggregation Functions\n",
    "\n",
    "Aggregate findings to control-level and account/region summaries with compliance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_findings_to_controls(findings_df):\n",
    "    \"\"\"\n",
    "    Aggregate findings to control-level status.\n",
    "    AWS Security Hub CSPM-compliant aggregation logic.\n",
    "    \"\"\"\n",
    "    # Normalize compliance status and severity\n",
    "    findings = (\n",
    "        findings_df\n",
    "        .withColumn(\"compliance_status\", F.upper(\"compliance_status\"))\n",
    "        .withColumn(\n",
    "            \"severity\",\n",
    "            F.when(F.col(\"severity\").isNull(), \"unclassified\")\n",
    "             .otherwise(F.lower(\"severity\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"is_suppressed\",\n",
    "            F.upper(F.col(\"finding_status\")) == F.lit(\"SUPPRESSED\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"severity_rank\",\n",
    "            F.when(F.col(\"severity\") == \"critical\", 4)\n",
    "             .when(F.col(\"severity\") == \"high\", 3)\n",
    "             .when(F.col(\"severity\") == \"medium\", 2)\n",
    "             .when(F.col(\"severity\") == \"low\", 1)\n",
    "             .otherwise(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Control-level aggregation\n",
    "    control_key = [\"account_id\", \"region_id\", \"standard_id\", \"control_id\"]\n",
    "\n",
    "    controls = (\n",
    "        findings\n",
    "        .groupBy(*control_key)\n",
    "        .agg(\n",
    "            # Count-based aggregation (CSPM-compliant)\n",
    "            F.sum(F.when(~F.col(\"is_suppressed\"), 1).otherwise(0)).alias(\"active_cnt\"),\n",
    "            F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\") == \"FAILED\"), 1).otherwise(0)).alias(\"failed_cnt\"),\n",
    "            F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\") == \"PASSED\"), 1).otherwise(0)).alias(\"passed_cnt\"),\n",
    "            F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\").isin(\"WARNING\", \"NOT_AVAILABLE\")), 1).otherwise(0)).alias(\"unknown_cnt\"),\n",
    "            F.count(\"*\").alias(\"total_cnt\"),\n",
    "            F.max(\"severity_rank\").alias(\"max_severity_rank\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"control_status\",\n",
    "            F.when(F.col(\"active_cnt\") == 0, \"NO_DATA\")\n",
    "             .when(F.col(\"failed_cnt\") > 0, \"FAILED\")\n",
    "             .when(F.col(\"unknown_cnt\") > 0, \"UNKNOWN\")\n",
    "             .when(F.col(\"passed_cnt\") == F.col(\"active_cnt\"), \"PASSED\")\n",
    "             .otherwise(\"UNKNOWN\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"severity\",\n",
    "            F.when(F.col(\"max_severity_rank\") == 4, \"critical\")\n",
    "             .when(F.col(\"max_severity_rank\") == 3, \"high\")\n",
    "             .when(F.col(\"max_severity_rank\") == 2, \"medium\")\n",
    "             .when(F.col(\"max_severity_rank\") == 1, \"low\")\n",
    "             .otherwise(\"unclassified\")\n",
    "        )\n",
    "        .drop(\"max_severity_rank\")\n",
    "    )\n",
    "\n",
    "    return controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb21b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_account_region_summary(controls_df, company_id):\n",
    "    \"\"\"\n",
    "    Aggregate control-level data to account/region summary.\n",
    "    Includes per-standard and per-severity breakdowns.\n",
    "    \"\"\"\n",
    "    std_key = [\"account_id\", \"region_id\", \"standard_id\"]\n",
    "\n",
    "    # Severity-level aggregation\n",
    "    severity_agg = (\n",
    "        controls_df\n",
    "        .groupBy(*std_key, \"severity\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"control_id\").alias(\"total\"),\n",
    "            F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"passed\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"score\",\n",
    "            F.round(\n",
    "                F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                 .otherwise(0.0),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Standard-level aggregation\n",
    "    standards = (\n",
    "        severity_agg\n",
    "        .groupBy(*std_key)\n",
    "        .agg(\n",
    "            F.sum(\"total\").alias(\"total\"),\n",
    "            F.sum(\"passed\").alias(\"passed\"),\n",
    "            F.collect_list(\n",
    "                F.struct(\n",
    "                    F.col(\"severity\").alias(\"level\"),\n",
    "                    \"score\",\n",
    "                    F.struct(\"total\", \"passed\").alias(\"controls\")\n",
    "                )\n",
    "            ).alias(\"controls_by_severity\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"score\",\n",
    "            F.round(\n",
    "                F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                 .otherwise(0.0),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "        .select(\n",
    "            *std_key,\n",
    "            F.struct(\n",
    "                F.col(\"standard_id\").alias(\"std\"),\n",
    "                \"score\",\n",
    "                F.struct(\"total\", \"passed\").alias(\"controls\"),\n",
    "                \"controls_by_severity\"\n",
    "            ).alias(\"standard_summary\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Account/region summary\n",
    "    region_key = [\"account_id\", \"region_id\"]\n",
    "\n",
    "    overall = (\n",
    "        controls_df\n",
    "        .groupBy(*region_key)\n",
    "        .agg(\n",
    "            F.countDistinct(F.struct(\"standard_id\", \"control_id\")).alias(\"total_rules\"),\n",
    "            F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"total_passed\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"control_pass_score\",\n",
    "            F.round(\n",
    "                F.when(F.col(\"total_rules\") > 0, (F.col(\"total_passed\") / F.col(\"total_rules\")) * 100)\n",
    "                 .otherwise(0.0),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join standards summary\n",
    "    standards_summary_df = (\n",
    "        overall\n",
    "        .join(\n",
    "            standards.groupBy(*region_key)\n",
    "                     .agg(F.collect_list(\"standard_summary\").alias(\"standards_summary\")),\n",
    "            region_key\n",
    "        )\n",
    "        .withColumn(\"cf_processed_time\", F.current_timestamp())\n",
    "        .withColumn(\"company_id\", F.lit(company_id))\n",
    "        .select(\n",
    "            \"company_id\",\n",
    "            \"cf_processed_time\",\n",
    "            \"account_id\",\n",
    "            \"region_id\",\n",
    "            \"control_pass_score\",\n",
    "            \"total_rules\",\n",
    "            \"total_passed\",\n",
    "            \"standards_summary\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return standards_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717f385",
   "metadata": {},
   "source": [
    "## Company Discovery\n",
    "\n",
    "Determine which companies to process based on job parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine companies to process\n",
    "if not company_index_id_param or company_index_id_param.upper() == \"ALL\":\n",
    "    companies_to_process = discover_companies(catalog_name)\n",
    "    print(f\"\\n[INFO] Auto-discovery mode: Found {len(companies_to_process)} companies\")\n",
    "    if companies_to_process:\n",
    "        if len(companies_to_process) <= 10:\n",
    "            print(f\"[INFO] Companies: {', '.join(companies_to_process)}\")\n",
    "        else:\n",
    "            print(f\"[INFO] Companies: {', '.join(companies_to_process[:10])}... and {len(companies_to_process) - 10} more\")\n",
    "else:\n",
    "    # Single company mode\n",
    "    if not is_valid_company_id(company_index_id_param):\n",
    "        raise ValueError(f\"Invalid company_id format: {company_index_id_param}. Must be 12 lowercase alphanumeric characters.\")\n",
    "    companies_to_process = [company_index_id_param]\n",
    "    print(f\"\\n[INFO] Single company mode: {company_index_id_param}\")\n",
    "\n",
    "print(f\"\\n[INFO] Total companies to process: {len(companies_to_process)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31c6b5",
   "metadata": {},
   "source": [
    "## Main Processing Function\n",
    "\n",
    "Load bronze data, transform to canonical format, aggregate summaries, and write directly to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROCESSING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def process_company(company_id, catalog_name, window_start_ts, window_end_ts, s3_base_path, date_str):\n",
    "    \"\"\"\n",
    "    Process a single company: load bronze → transform → aggregate → write to S3.\n",
    "    Writes data to: s3://bucket/company_id/aws/data_type/YYYY-MM-DD.csv.gz\n",
    "    Returns tuple: (success: bool, message: str, stats: dict)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PROCESSING COMPANY: {company_id}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        # Define bronze table names for this company\n",
    "        asff_tbl = f\"{catalog_name}.{company_id}.aws_securityhub_findings_1_0\"\n",
    "        ocsf_tbl = f\"{catalog_name}.{company_id}.aws_securitylake_sh_findings_2_0\"\n",
    "\n",
    "        print(f\"[TABLE] ASFF Bronze:   {asff_tbl}\")\n",
    "        print(f\"[TABLE] OCSF Bronze:   {ocsf_tbl}\")\n",
    "        print(f\"[OUTPUT] S3 Path:      {s3_base_path}/{company_id}/aws/\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        # ============================================================\n",
    "        # BRONZE → IN-MEMORY: Load and Transform\n",
    "        # ============================================================\n",
    "\n",
    "        # Check table existence\n",
    "        asff_exists = table_exists(asff_tbl)\n",
    "        ocsf_exists = table_exists(ocsf_tbl)\n",
    "\n",
    "        print(f\"[CHECK] ASFF table exists: {asff_exists}\")\n",
    "        print(f\"[CHECK] OCSF table exists: {ocsf_exists}\")\n",
    "\n",
    "        if not asff_exists and not ocsf_exists:\n",
    "            print(f\"[SKIP] Neither bronze table exists for {company_id}\")\n",
    "            return (False, \"No bronze tables\", {})\n",
    "\n",
    "        sources = []\n",
    "\n",
    "        # Load ASFF data\n",
    "        if asff_exists:\n",
    "            df_asff_raw = (\n",
    "                spark.table(asff_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"product_name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            asff_count = df_asff_raw.count()\n",
    "            print(f\"[DATA] ASFF rows in window: {asff_count:,}\")\n",
    "            if asff_count > 0:\n",
    "                sources.append((\"ASFF\", df_asff_raw))\n",
    "\n",
    "        # Load OCSF data\n",
    "        if ocsf_exists:\n",
    "            df_ocsf_raw = (\n",
    "                spark.table(ocsf_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"metadata.product.name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            ocsf_count = df_ocsf_raw.count()\n",
    "            print(f\"[DATA] OCSF rows in window: {ocsf_count:,}\")\n",
    "            if ocsf_count > 0:\n",
    "                sources.append((\"OCSF\", df_ocsf_raw))\n",
    "\n",
    "        if len(sources) == 0:\n",
    "            print(f\"[SKIP] No data found in 48-hour window for {company_id}\")\n",
    "            return (False, \"No data in window\", {})\n",
    "\n",
    "        # Transform and union\n",
    "        canonical_dfs = []\n",
    "        for src, df_raw in sources:\n",
    "            if src == \"ASFF\":\n",
    "                out = transform_asff(df_raw)\n",
    "            elif src == \"OCSF\":\n",
    "                out = transform_ocsf(df_raw)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            out = out.withColumn(\"finding_id\", normalize_finding_id(F.col(\"finding_id\"))) \\\n",
    "                     .where(F.col(\"finding_id\").isNotNull())\n",
    "            canonical_dfs.append(out)\n",
    "\n",
    "        if not canonical_dfs:\n",
    "            print(f\"[SKIP] No valid findings after filtering for {company_id}\")\n",
    "            return (False, \"No valid findings\", {})\n",
    "\n",
    "        df_union = canonical_dfs[0]\n",
    "        for d in canonical_dfs[1:]:\n",
    "            df_union = df_union.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "        union_count = df_union.count()\n",
    "        print(f\"[TRANSFORM] Union rows: {union_count:,}\")\n",
    "\n",
    "        # Deduplicate\n",
    "        w = Window.partitionBy(\"finding_id\").orderBy(\n",
    "            F.col(\"finding_modified_time\").desc_nulls_last(),\n",
    "            F.col(\"_preference\").desc(),\n",
    "            F.col(\"_bronze_processed_time\").desc_nulls_last()\n",
    "        )\n",
    "\n",
    "        findings = (\n",
    "            df_union\n",
    "            .withColumn(\"_rn\", F.row_number().over(w))\n",
    "            .where(F.col(\"_rn\") == 1)\n",
    "            .drop(\"_rn\", \"_preference\", \"_bronze_processed_time\")\n",
    "        )\n",
    "\n",
    "        findings_count = findings.count()\n",
    "        print(f\"[TRANSFORM] Deduplicated findings: {findings_count:,}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # APPLY CORRECT SEVERITY FROM REFERENCE TABLE\n",
    "        # ============================================================\n",
    "\n",
    "        if controls_ref_df is not None:\n",
    "            # Left join with reference table to get correct severity\n",
    "            findings = (\n",
    "                findings\n",
    "                .join(\n",
    "                    controls_ref_df.withColumnRenamed(\"severity\", \"ref_severity\"),\n",
    "                    on=\"control_id\",\n",
    "                    how=\"left\"\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"severity\",\n",
    "                    # Use reference severity if available, otherwise keep original\n",
    "                    F.when(F.col(\"ref_severity\").isNotNull(), F.lower(F.col(\"ref_severity\")))\n",
    "                     .otherwise(F.lower(F.col(\"severity\")))\n",
    "                )\n",
    "                .drop(\"ref_severity\")\n",
    "            )\n",
    "\n",
    "            print(f\"[SEVERITY] Applied reference table corrections (control_id -> severity)\")\n",
    "\n",
    "        else:\n",
    "            print(f\"[SEVERITY] Using severity from source data (no reference table)\")\n",
    "\n",
    "        # ============================================================\n",
    "        # IN-MEMORY → OUTPUT: Aggregate and Prepare DataFrames\n",
    "        # ============================================================\n",
    "\n",
    "        # Aggregate to control-level\n",
    "        controls = aggregate_findings_to_controls(findings)\n",
    "\n",
    "        active_findings = findings.where(F.upper(F.col(\"finding_status\")) != F.lit(\"SUPPRESSED\"))\n",
    "        active_count = active_findings.count()\n",
    "        suppressed_count = findings_count - active_count\n",
    "        print(f\"[AGGREGATE] Active findings: {active_count:,}\")\n",
    "        print(f\"[AGGREGATE] Suppressed findings: {suppressed_count:,}\")\n",
    "\n",
    "        # Aggregate to account/region summary\n",
    "        standards_summary_df = aggregate_account_region_summary(controls, company_id)\n",
    "\n",
    "        standards_summary_count = standards_summary_df.count()\n",
    "        print(f\"[AGGREGATE] Standards summary rows: {standards_summary_count}\")\n",
    "\n",
    "        if standards_summary_count > 0:\n",
    "            # ============================================================\n",
    "            # WRITE TO S3: Standards Summary (JSONL)\n",
    "            # ============================================================\n",
    "\n",
    "            # S3 path: company_id/aws/standards_summary/YYYY-MM-DD/\n",
    "            # JSONL format: standards_summary is already JSON, no need to convert\n",
    "            standards_s3_path = f\"{s3_base_path}/{company_id}/aws/standards_summary/{date_str}\"\n",
    "\n",
    "            print(f\"[WRITE] Standards summary → {standards_s3_path} (JSONL)\")\n",
    "            (standards_summary_df\n",
    "             .coalesce(1)  # Single file per company\n",
    "             .write\n",
    "             .mode(\"overwrite\")\n",
    "             .option(\"compression\", \"gzip\")\n",
    "             .json(standards_s3_path))\n",
    "\n",
    "            print(f\"[SUCCESS] Wrote {standards_summary_count} standards summary rows (JSONL)\")\n",
    "\n",
    "            # ============================================================\n",
    "            # AGGREGATE & WRITE: Account Compliance Summary (JSONL)\n",
    "            # ============================================================\n",
    "\n",
    "            # Aggregate regional data to account level\n",
    "            account_summary = (\n",
    "                standards_summary_df\n",
    "                .groupBy(\"company_id\", \"account_id\")\n",
    "                .agg(\n",
    "                    F.sum(\"total_rules\").cast(\"int\").alias(\"total_rules\"),\n",
    "                    F.sum(\"total_passed\").cast(\"int\").alias(\"total_passed\"),\n",
    "                    F.first(\"cf_processed_time\").alias(\"cf_processed_time\")\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"score\",\n",
    "                    F.round(\n",
    "                        F.when(F.col(\"total_rules\") > 0, (F.col(\"total_passed\") / F.col(\"total_rules\")) * 100)\n",
    "                         .otherwise(0.0),\n",
    "                        7\n",
    "                    )\n",
    "                )\n",
    "                .select(\n",
    "                    \"company_id\",\n",
    "                    \"cf_processed_time\",\n",
    "                    \"account_id\",\n",
    "                    \"score\",\n",
    "                    \"total_rules\",\n",
    "                    \"total_passed\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            account_count = account_summary.count()\n",
    "            print(f\"[AGGREGATE] Account-level summary rows: {account_count}\")\n",
    "\n",
    "            if account_count > 0:\n",
    "                # S3 path: company_id/aws/account_compliance_summary/YYYY-MM-DD/\n",
    "                account_s3_path = f\"{s3_base_path}/{company_id}/aws/account_compliance_summary/{date_str}\"\n",
    "\n",
    "                print(f\"[WRITE] Account compliance summary → {account_s3_path} (JSONL)\")\n",
    "                (account_summary\n",
    "                 .coalesce(1)  # Single file per company\n",
    "                 .write\n",
    "                 .mode(\"overwrite\")\n",
    "                 .option(\"compression\", \"gzip\")\n",
    "                 .json(account_s3_path))\n",
    "\n",
    "                print(f\"[SUCCESS] Wrote {account_count} account compliance rows (JSONL)\")\n",
    "\n",
    "                return (True, \"Success\", {\n",
    "                    \"standards_rows\": standards_summary_count,\n",
    "                    \"account_rows\": account_count\n",
    "                })\n",
    "            else:\n",
    "                print(f\"[SUCCESS] Standards summary written (no account-level data)\")\n",
    "                return (True, \"Success\", {\n",
    "                    \"standards_rows\": standards_summary_count,\n",
    "                    \"account_rows\": 0\n",
    "                })\n",
    "        else:\n",
    "            print(f\"[SKIP] No summary data generated for {company_id}\")\n",
    "            return (False, \"No summary data\", {})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {company_id}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return (False, str(e), {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db3eab",
   "metadata": {},
   "source": [
    "## Execute Pipeline\n",
    "\n",
    "Process each company and write directly to S3 in JSONL format: `company_id/aws/data_type/YYYY-MM-DD/part-*.json.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXECUTE PIPELINE: Process Each Company\n",
    "# ============================================================\n",
    "\n",
    "# Track results\n",
    "successful_companies = []\n",
    "failed_companies = []\n",
    "skipped_companies = []\n",
    "total_stats = {\"standards_rows\": 0, \"account_rows\": 0}\n",
    "\n",
    "# Process each company individually and write to S3\n",
    "for company_id in companies_to_process:\n",
    "    success, message, stats = process_company(\n",
    "        company_id, catalog_name, window_start_ts, window_end_ts, s3_base_path, date_str\n",
    "    )\n",
    "\n",
    "    if success:\n",
    "        successful_companies.append(company_id)\n",
    "        total_stats[\"standards_rows\"] += stats.get(\"standards_rows\", 0)\n",
    "        total_stats[\"account_rows\"] += stats.get(\"account_rows\", 0)\n",
    "    elif message in [\"No bronze tables\", \"No data in window\", \"No valid findings\", \"No summary data\"]:\n",
    "        skipped_companies.append((company_id, message))\n",
    "    else:\n",
    "        failed_companies.append((company_id, message))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total companies processed:  {len(companies_to_process)}\")\n",
    "print(f\"  - Successful:             {len(successful_companies)}\")\n",
    "print(f\"  - Skipped:                {len(skipped_companies)}\")\n",
    "print(f\"  - Failed:                 {len(failed_companies)}\")\n",
    "print(f\"\\nData written:\")\n",
    "print(f\"  - Standards summary rows: {total_stats['standards_rows']:,}\")\n",
    "print(f\"  - Account summary rows:   {total_stats['account_rows']:,}\")\n",
    "print(f\"\\nOutput Format: JSONL (JSON Lines) with gzip compression\")\n",
    "print(f\"S3 structure: {s3_base_path}/company_id/aws/data_type/{date_str}/\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcf1db",
   "metadata": {},
   "source": [
    "## Summary Report\n",
    "\n",
    "Display processing results and raise exception if any failures occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECURITY HUB STANDARDS ETL PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Companies:    {len(companies_to_process)}\")\n",
    "print(f\"[SUCCESS]           {len(successful_companies)}\")\n",
    "print(f\"[SKIPPED]           {len(skipped_companies)}\")\n",
    "print(f\"[FAILED]            {len(failed_companies)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if successful_companies:\n",
    "    print(f\"\\n[SUCCESS] COMPLETED COMPANIES ({len(successful_companies)}):\")\n",
    "    for comp_id in successful_companies:\n",
    "        print(f\"  - {comp_id}\")\n",
    "\n",
    "if skipped_companies:\n",
    "    print(f\"\\n[SKIPPED] COMPANIES ({len(skipped_companies)}):\")\n",
    "    for comp_id, reason in skipped_companies:\n",
    "        print(f\"  - [{comp_id}] {reason}\")\n",
    "\n",
    "if failed_companies:\n",
    "    print(f\"\\n[FAILED] COMPANIES ({len(failed_companies)}):\")\n",
    "    for comp_id, error in failed_companies:\n",
    "        error_msg = str(error) if error else \"Unknown error\"\n",
    "        print(f\"  - [{comp_id}] {error_msg[:150]}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    raise Exception(f\"Pipeline failed for {len(failed_companies)} company(ies). See error details above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[SUCCESS] SECURITY HUB STANDARDS ETL COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
