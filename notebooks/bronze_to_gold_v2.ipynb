{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d966407-ce25-43b8-914f-ea79ba4c33a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Security Hub Standards ETL Pipeline\n",
    "\n",
    "AWS Security Hub compliance data processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a0f79c-94b8-48da-8b3c-35a7fe075c63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration & Setup\n",
    "\n",
    "Load parameters, configure Spark, and define the processing time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac834d61-191a-4514-a6b1-406ff16593f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import traceback\n",
    "\n",
    "# Get parameters from job\n",
    "dbutils.widgets.text(\"CATALOG_NAME\", \"\")\n",
    "dbutils.widgets.text(\"COMPANY_INDEX_ID\", \"\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"CATALOG_NAME\").strip()\n",
    "company_index_id_param = dbutils.widgets.get(\"COMPANY_INDEX_ID\").strip()\n",
    "\n",
    "if not catalog_name:\n",
    "    raise ValueError(\"Missing required param: CATALOG_NAME\")\n",
    "\n",
    "# Set timezone\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "# Calculate job date and processing window\n",
    "# 48-hour window ensures we capture all findings (Security Hub checks every 18 hours)\n",
    "job_date = F.date_trunc(\"DAY\", F.current_timestamp())\n",
    "window_end_ts = job_date\n",
    "window_start_ts = window_end_ts - F.expr(\"INTERVAL 48 HOURS\")\n",
    "cf_processed_time = job_date\n",
    "\n",
    "# Determine processing mode\n",
    "is_all_companies = not company_index_id_param or company_index_id_param.upper() == \"ALL\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECURITY HUB STANDARDS ETL PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Catalog:            {catalog_name}\")\n",
    "print(f\"Company Mode:       {'Auto-Discovery (ALL)' if is_all_companies else company_index_id_param}\")\n",
    "print(f\"Job Date:           {job_date}\")\n",
    "print(f\"Time Window:        48 hours (Security Hub check cycle: 18 hours)\")\n",
    "print(f\"Retention Strategy: 1-day (TRUNCATE + Append)\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31dd692c-1371-4656-948d-df60382b1de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions for company ID validation, table existence checks, and company discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2de7dd4-3a61-4146-9f52-eb25e46253bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_valid_company_id(schema_name: str) -> bool:\n",
    "    \"\"\"Check if schema name matches company ID format: 12 chars, lowercase alphanumeric.\"\"\"\n",
    "    return (\n",
    "        len(schema_name) == 12 and\n",
    "        schema_name.isalnum() and\n",
    "        schema_name.islower()\n",
    "    )\n",
    "\n",
    "def table_exists(full_name: str) -> bool:\n",
    "    \"\"\"Check if a table exists in the catalog.\"\"\"\n",
    "    try:\n",
    "        return spark.catalog.tableExists(full_name)\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db79f7aa-015f-4012-9ea3-c9696a0406a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def discover_companies(catalog: str) -> list:\n",
    "    \"\"\"Discover all company schemas in the catalog.\"\"\"\n",
    "    try:\n",
    "        # Unity Catalog: need to list schemas within the catalog\n",
    "        print(f\"[DEBUG] Searching for companies in catalog: {catalog}\")\n",
    "\n",
    "        # Set current catalog and list schemas\n",
    "        spark.sql(f\"USE CATALOG {catalog}\")\n",
    "        schemas = spark.catalog.listDatabases()\n",
    "\n",
    "        print(f\"[DEBUG] Total schemas found in {catalog}: {len(schemas)}\")\n",
    "\n",
    "        companies = []\n",
    "        for schema in schemas:\n",
    "            schema_name = schema.name\n",
    "            print(f\"[DEBUG] Checking schema: {schema_name}\")\n",
    "\n",
    "            # In Unity Catalog, schema.name is just the schema name (not catalog.schema)\n",
    "            # But might still have catalog prefix in some cases\n",
    "            if '.' in schema_name:\n",
    "                # Handle \"catalog.schema\" format\n",
    "                parts = schema_name.split('.')\n",
    "                if parts[0] == catalog and len(parts) == 2:\n",
    "                    schema_name = parts[1]\n",
    "                    print(f\"[DEBUG]   -> Extracted schema: {schema_name}\")\n",
    "                else:\n",
    "                    print(f\"[DEBUG]   -> Skipped (unexpected format: {schema_name})\")\n",
    "                    continue\n",
    "\n",
    "            if is_valid_company_id(schema_name):\n",
    "                print(f\"[DEBUG]   -> ✓ Valid company ID: {schema_name}\")\n",
    "                companies.append(schema_name)\n",
    "            else:\n",
    "                print(f\"[DEBUG]   -> ✗ Invalid company ID format: {schema_name} (len={len(schema_name)}, alnum={schema_name.isalnum()}, lower={schema_name.islower()})\")\n",
    "\n",
    "        return sorted(companies)\n",
    "    except Exception as e:\n",
    "        print(f\"Error discovering companies: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54bed38b-7e6f-4679-94fe-a67ceea826bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_finding_id(col):\n",
    "    \"\"\"Normalize finding ID: trim and convert empty to NULL.\"\"\"\n",
    "    return F.when(F.length(F.trim(col)) == 0, F.lit(None)).otherwise(F.trim(col))\n",
    "\n",
    "def parse_iso8601_to_ts(col):\n",
    "    \"\"\"Parse ISO8601 timestamp string to Spark timestamp.\"\"\"\n",
    "    return F.to_timestamp(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c065e36-2ca7-4e74-a263-8e6dcf146c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Security Hub Controls Reference\n",
    "\n",
    "Load the reference table mapping control IDs to correct severity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4f06df-64c9-480d-b967-54fe4993a2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD SECURITY HUB CONTROLS REFERENCE TABLE\n",
    "# ============================================================\n",
    "\n",
    "# Load the reference table with correct control_id -> severity mappings\n",
    "controls_ref_table = f\"{catalog_name}.reference.securityhub_controls\"\n",
    "\n",
    "try:\n",
    "    if table_exists(controls_ref_table):\n",
    "        controls_ref_df = spark.table(controls_ref_table).select(\"control_id\", \"severity\")\n",
    "        ref_count = controls_ref_df.count()\n",
    "        print(f\"[INFO] Loaded Security Hub controls reference table: {ref_count} mappings\")\n",
    "    else:\n",
    "        print(f\"[WARN] Reference table {controls_ref_table} not found. Severity will be taken from source data.\")\n",
    "        controls_ref_df = None\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not load reference table: {e}. Severity will be taken from source data.\")\n",
    "    controls_ref_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf4dfb74-1da5-4dc0-8b58-0ff90f41f0da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Transformation Functions\n",
    "\n",
    "Transform ASFF and OCSF formats to canonical schema, excluding archived findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2037d7d-afcd-4f4c-b205-143c11c42bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_asff(df):\n",
    "    \"\"\"\n",
    "    Transform ASFF (AWS Security Finding Format) to canonical schema.\n",
    "    Excludes ARCHIVED findings and normalizes fields.\n",
    "    Preserves original workflow status values: NEW, NOTIFIED, SUPPRESSED, RESOLVED.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .where(F.col(\"RecordState\") != \"ARCHIVED\")\n",
    "        .select(\n",
    "            normalize_finding_id(F.col(\"finding_id\")).alias(\"finding_id\"),\n",
    "            parse_iso8601_to_ts(F.col(\"updated_at\")).alias(\"finding_modified_time\"),\n",
    "            F.upper(F.col(\"workflow.Status\")).alias(\"finding_status\"),\n",
    "            F.col(\"aws_account_id\").cast(\"string\").alias(\"account_id\"),\n",
    "            F.col(\"finding_region\").cast(\"string\").alias(\"region_id\"),\n",
    "            F.expr(\"compliance.AssociatedStandards[0].StandardsId\").cast(\"string\").alias(\"standard_id\"),\n",
    "            F.col(\"compliance.SecurityControlId\").cast(\"string\").alias(\"control_id\"),\n",
    "            F.col(\"compliance.Status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "            F.col(\"severity.Label\").cast(\"string\").alias(\"severity\"),\n",
    "            F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "            F.lit(1).alias(\"_preference\")  # ASFF preferred over OCSF\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "597ab2f6-c8b1-480a-9d56-a44a4405f154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_ocsf(df):\n",
    "    \"\"\"\n",
    "    Transform OCSF (Open Cybersecurity Schema Framework) to canonical schema.\n",
    "    Excludes ARCHIVED findings and normalizes fields.\n",
    "    Maps workflow status to match ASFF format: NEW, NOTIFIED, SUPPRESSED, RESOLVED.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .where(F.col(\"unmapped.RecordState\") != \"ARCHIVED\")\n",
    "        .select(\n",
    "            normalize_finding_id(F.col(\"finding_info.uid\")).alias(\"finding_id\"),\n",
    "            parse_iso8601_to_ts(F.col(\"finding_info.modified_time_dt\")).alias(\"finding_modified_time\"),\n",
    "            # Map OCSF workflow states to match ASFF uppercase format\n",
    "            F.when(F.col(\"unmapped.WorkflowState\").isNotNull(), F.upper(F.col(\"unmapped.WorkflowState\")))\n",
    "             .when(F.upper(F.col(\"status\").cast(\"string\")) == \"IN_PROGRESS\", \"NOTIFIED\")\n",
    "             .otherwise(F.upper(F.col(\"status\").cast(\"string\")))\n",
    "             .alias(\"finding_status\"),\n",
    "            F.col(\"cloud.account.uid\").cast(\"string\").alias(\"account_id\"),\n",
    "            F.col(\"cloud.region\").cast(\"string\").alias(\"region_id\"),\n",
    "            F.expr(\"compliance.standards[0]\").cast(\"string\").alias(\"standard_id\"),\n",
    "            F.col(\"compliance.control\").cast(\"string\").alias(\"control_id\"),\n",
    "            F.col(\"compliance.status\").cast(\"string\").alias(\"compliance_status\"),\n",
    "            F.col(\"severity\").cast(\"string\").alias(\"severity\"),\n",
    "            F.col(\"cf_processed_time\").alias(\"_bronze_processed_time\"),\n",
    "            F.lit(0).alias(\"_preference\")  # OCSF fallback\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd6afaa-a31b-49ab-bb23-89bc72643ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Aggregation Functions\n",
    "\n",
    "Aggregate findings to control-level and account/region summaries with compliance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c5cca6-62d6-4ece-83b5-3ae9dfa4a8de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_findings_to_controls(findings_df):\n",
    "    \"\"\"\n",
    "    Aggregate findings to control-level status.\n",
    "    AWS Security Hub CSPM-compliant aggregation logic.\n",
    "    \"\"\"\n",
    "    # Normalize compliance status and severity\n",
    "    findings = (\n",
    "        findings_df\n",
    "        .withColumn(\"compliance_status\", F.upper(\"compliance_status\"))\n",
    "        .withColumn(\n",
    "            \"severity\",\n",
    "            F.when(F.col(\"severity\").isNull(), \"unclassified\")\n",
    "             .otherwise(F.lower(\"severity\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"is_suppressed\",\n",
    "            F.upper(F.col(\"finding_status\")) == F.lit(\"SUPPRESSED\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"severity_rank\",\n",
    "            F.when(F.col(\"severity\") == \"critical\", 4)\n",
    "             .when(F.col(\"severity\") == \"high\", 3)\n",
    "             .when(F.col(\"severity\") == \"medium\", 2)\n",
    "             .when(F.col(\"severity\") == \"low\", 1)\n",
    "             .otherwise(0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Control-level aggregation\n",
    "    control_key = [\"account_id\", \"region_id\", \"standard_id\", \"control_id\"]\n",
    "\n",
    "    controls = (\n",
    "        findings\n",
    "        .groupBy(*control_key)\n",
    "        .agg(\n",
    "            # Count-based aggregation (CSPM-compliant)\n",
    "            F.sum(F.when(~F.col(\"is_suppressed\"), 1).otherwise(0)).alias(\"active_cnt\"),\n",
    "            F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\") == \"FAILED\"), 1).otherwise(0)).alias(\"failed_cnt\"),\n",
    "            F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\") == \"PASSED\"), 1).otherwise(0)).alias(\"passed_cnt\"),\n",
    "            F.sum(F.when((~F.col(\"is_suppressed\")) & (F.col(\"compliance_status\").isin(\"WARNING\", \"NOT_AVAILABLE\")), 1).otherwise(0)).alias(\"unknown_cnt\"),\n",
    "            F.count(\"*\").alias(\"total_cnt\"),\n",
    "            F.max(\"severity_rank\").alias(\"max_severity_rank\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"control_status\",\n",
    "            F.when(F.col(\"active_cnt\") == 0, \"NO_DATA\")\n",
    "             .when(F.col(\"failed_cnt\") > 0, \"FAILED\")\n",
    "             .when(F.col(\"unknown_cnt\") > 0, \"UNKNOWN\")\n",
    "             .when(F.col(\"passed_cnt\") == F.col(\"active_cnt\"), \"PASSED\")\n",
    "             .otherwise(\"UNKNOWN\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"severity\",\n",
    "            F.when(F.col(\"max_severity_rank\") == 4, \"critical\")\n",
    "             .when(F.col(\"max_severity_rank\") == 3, \"high\")\n",
    "             .when(F.col(\"max_severity_rank\") == 2, \"medium\")\n",
    "             .when(F.col(\"max_severity_rank\") == 1, \"low\")\n",
    "             .otherwise(\"unclassified\")\n",
    "        )\n",
    "        .drop(\"max_severity_rank\")\n",
    "    )\n",
    "\n",
    "    return controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81a827ea-3364-4f19-97cc-0707c76b591f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_account_region_summary(controls_df, company_id, cf_processed_time):\n",
    "    \"\"\"\n",
    "    Aggregate control-level data to account/region summary.\n",
    "    Includes per-standard and per-severity breakdowns.\n",
    "    \"\"\"\n",
    "    std_key = [\"account_id\", \"region_id\", \"standard_id\"]\n",
    "\n",
    "    # Severity-level aggregation\n",
    "    severity_agg = (\n",
    "        controls_df\n",
    "        .groupBy(*std_key, \"severity\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"control_id\").alias(\"total\"),\n",
    "            F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"passed\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"score\",\n",
    "            F.round(\n",
    "                F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                 .otherwise(0.0),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Standard-level aggregation\n",
    "    standards = (\n",
    "        severity_agg\n",
    "        .groupBy(*std_key)\n",
    "        .agg(\n",
    "            F.sum(\"total\").alias(\"total\"),\n",
    "            F.sum(\"passed\").alias(\"passed\"),\n",
    "            F.collect_list(\n",
    "                F.struct(\n",
    "                    F.col(\"severity\").alias(\"level\"),\n",
    "                    \"score\",\n",
    "                    F.struct(\"total\", \"passed\").alias(\"controls\")\n",
    "                )\n",
    "            ).alias(\"controls_by_severity\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"score\",\n",
    "            F.round(\n",
    "                F.when(F.col(\"total\") > 0, F.col(\"passed\") * 100.0 / F.col(\"total\"))\n",
    "                 .otherwise(0.0),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "        .select(\n",
    "            *std_key,\n",
    "            F.struct(\n",
    "                F.col(\"standard_id\").alias(\"std\"),\n",
    "                \"score\",\n",
    "                F.struct(\"total\", \"passed\").alias(\"controls\"),\n",
    "                \"controls_by_severity\"\n",
    "            ).alias(\"standard_summary\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Account/region summary\n",
    "    region_key = [\"account_id\", \"region_id\"]\n",
    "\n",
    "    overall = (\n",
    "        controls_df\n",
    "        .groupBy(*region_key)\n",
    "        .agg(\n",
    "            F.countDistinct(F.struct(\"standard_id\", \"control_id\")).alias(\"total_rules\"),\n",
    "            F.sum(F.when(F.col(\"control_status\") == \"PASSED\", 1).otherwise(0)).cast(\"int\").alias(\"total_passed\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"control_pass_score\",\n",
    "            F.round(\n",
    "                F.when(F.col(\"total_rules\") > 0, (F.col(\"total_passed\") / F.col(\"total_rules\")) * 100)\n",
    "                 .otherwise(0.0),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join standards summary\n",
    "    standards_summary_df = (\n",
    "        overall\n",
    "        .join(\n",
    "            standards.groupBy(*region_key)\n",
    "                     .agg(F.collect_list(\"standard_summary\").alias(\"standards_summary\")),\n",
    "            region_key\n",
    "        )\n",
    "        .withColumn(\"cf_processed_time\", F.to_timestamp(F.lit(cf_processed_time)))\n",
    "        .withColumn(\"company_id\", F.lit(company_id))\n",
    "        .select(\n",
    "            \"company_id\",\n",
    "            \"cf_processed_time\",\n",
    "            \"account_id\",\n",
    "            \"region_id\",\n",
    "            \"control_pass_score\",\n",
    "            \"total_rules\",\n",
    "            \"total_passed\",\n",
    "            \"standards_summary\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return standards_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7a365f9-cd46-4506-8017-010f427f7fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Company Discovery\n",
    "\n",
    "Determine which companies to process based on job parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a17effee-370c-43bb-81d9-852cedd2a5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine companies to process\n",
    "if not company_index_id_param or company_index_id_param.upper() == \"ALL\":\n",
    "    companies_to_process = discover_companies(catalog_name)\n",
    "    print(f\"\\n[INFO] Auto-discovery mode: Found {len(companies_to_process)} companies\")\n",
    "    if companies_to_process:\n",
    "        if len(companies_to_process) <= 10:\n",
    "            print(f\"[INFO] Companies: {', '.join(companies_to_process)}\")\n",
    "        else:\n",
    "            print(f\"[INFO] Companies: {', '.join(companies_to_process[:10])}... and {len(companies_to_process) - 10} more\")\n",
    "else:\n",
    "    # Single company mode\n",
    "    if not is_valid_company_id(company_index_id_param):\n",
    "        raise ValueError(f\"Invalid company_id format: {company_index_id_param}. Must be 12 lowercase alphanumeric characters.\")\n",
    "    companies_to_process = [company_index_id_param]\n",
    "    print(f\"\\n[INFO] Single company mode: {company_index_id_param}\")\n",
    "\n",
    "print(f\"\\n[INFO] Total companies to process: {len(companies_to_process)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cf35f88-76e7-4a18-adc8-fe35994a5135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Schema Definitions\n",
    "\n",
    "Define output table schemas for standards summary and account compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8abea3-01cd-4be5-aa10-5769733a9809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_output_tables_exist(catalog_name, company_id):\n",
    "    \"\"\"\n",
    "    Ensure all output tables exist for a company.\n",
    "    Creates tables if they don't exist, does nothing if they do.\n",
    "    \"\"\"\n",
    "    # Standards summary table (account + region + standards breakdown)\n",
    "    standards_summary_tbl = f\"{catalog_name}.{company_id}.aws_standard_summary\"\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {standards_summary_tbl} (\n",
    "      company_id STRING,\n",
    "      cf_processed_time TIMESTAMP,\n",
    "      account_id STRING,\n",
    "      region_id STRING,\n",
    "      control_pass_score FLOAT,\n",
    "      total_rules INT,\n",
    "      total_passed INT,\n",
    "      standards_summary ARRAY<STRUCT<\n",
    "        std: STRING,\n",
    "        score: FLOAT,\n",
    "        controls: STRUCT<\n",
    "          total: INT,\n",
    "          passed: INT\n",
    "        >,\n",
    "        controls_by_severity: ARRAY<STRUCT<\n",
    "          level: STRING,\n",
    "          score: FLOAT,\n",
    "          controls: STRUCT<\n",
    "            total: INT,\n",
    "            passed: INT\n",
    "          >\n",
    "        >>\n",
    "      >>\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Regional standards compliance summary with per-standard and per-severity breakdowns'\n",
    "    \"\"\")\n",
    "\n",
    "    # Account compliance summary table (account-level aggregation)\n",
    "    account_summary_tbl = f\"{catalog_name}.{company_id}.aws_account_compliance_summary\"\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {account_summary_tbl} (\n",
    "      company_id STRING COMMENT 'データが属する企業のID',\n",
    "      cf_processed_time TIMESTAMP COMMENT '集計日 (Jobの実行日時)',\n",
    "      account_id STRING COMMENT 'アカウントID',\n",
    "      score FLOAT COMMENT 'コントロールに基づく総合スコア (passed_rules / total_rules * 100)',\n",
    "      total_rules INT COMMENT '適用コントロール総数 (passed + failed + unknown)',\n",
    "      total_passed INT COMMENT '合格コントロール数'\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Account-level compliance summary aggregated across all regions'\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"[SCHEMA] Ensured tables exist for company {company_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9974ec2c-4dca-4169-ab24-d154a8138aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Main Processing Function\n",
    "\n",
    "Load bronze data, transform to canonical format, aggregate summaries, and write results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8c3d6ab-75b2-47dd-a988-297893051993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROCESSING FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def process_company(company_id, catalog_name, window_start_ts, window_end_ts, cf_processed_time):\n",
    "    \"\"\"\n",
    "    Process a single company: load bronze → transform → aggregate → write summaries.\n",
    "    Returns tuple: (success: bool, message: str)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PROCESSING COMPANY: {company_id}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        # Define table names for this company\n",
    "        asff_tbl = f\"{catalog_name}.{company_id}.aws_securityhub_findings_1_0\"\n",
    "        ocsf_tbl = f\"{catalog_name}.{company_id}.aws_securitylake_sh_findings_2_0\"\n",
    "        standards_summary_tbl = f\"{catalog_name}.{company_id}.aws_standard_summary\"\n",
    "\n",
    "        print(f\"[TABLE] ASFF Bronze:   {asff_tbl}\")\n",
    "        print(f\"[TABLE] OCSF Bronze:   {ocsf_tbl}\")\n",
    "        print(f\"[TABLE] Standards Summary:  {standards_summary_tbl}\")\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        # ============================================================\n",
    "        # BRONZE → IN-MEMORY: Load and Transform\n",
    "        # ============================================================\n",
    "\n",
    "        # Check table existence\n",
    "        asff_exists = table_exists(asff_tbl)\n",
    "        ocsf_exists = table_exists(ocsf_tbl)\n",
    "\n",
    "        print(f\"[CHECK] ASFF table exists: {asff_exists}\")\n",
    "        print(f\"[CHECK] OCSF table exists: {ocsf_exists}\")\n",
    "\n",
    "        if not asff_exists and not ocsf_exists:\n",
    "            print(f\"[SKIP] Neither bronze table exists for {company_id}\")\n",
    "            return (False, \"No bronze tables\")\n",
    "\n",
    "        sources = []\n",
    "\n",
    "        # Load ASFF data\n",
    "        if asff_exists:\n",
    "            df_asff_raw = (\n",
    "                spark.table(asff_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"product_name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            asff_count = df_asff_raw.count()\n",
    "            print(f\"[DATA] ASFF rows in window: {asff_count:,}\")\n",
    "            if asff_count > 0:\n",
    "                sources.append((\"ASFF\", df_asff_raw))\n",
    "\n",
    "        # Load OCSF data\n",
    "        if ocsf_exists:\n",
    "            df_ocsf_raw = (\n",
    "                spark.table(ocsf_tbl)\n",
    "                .where(\n",
    "                    (F.col(\"metadata.product.name\") == \"Security Hub\") &\n",
    "                    (F.col(\"cf_processed_time\") >= window_start_ts) &\n",
    "                    (F.col(\"cf_processed_time\") < window_end_ts)\n",
    "                )\n",
    "            )\n",
    "            ocsf_count = df_ocsf_raw.count()\n",
    "            print(f\"[DATA] OCSF rows in window: {ocsf_count:,}\")\n",
    "            if ocsf_count > 0:\n",
    "                sources.append((\"OCSF\", df_ocsf_raw))\n",
    "\n",
    "        if len(sources) == 0:\n",
    "            print(f\"[SKIP] No data found in 48-hour window for {company_id}\")\n",
    "            return (False, \"No data in window\")\n",
    "\n",
    "        # Transform and union\n",
    "        canonical_dfs = []\n",
    "        for src, df_raw in sources:\n",
    "            if src == \"ASFF\":\n",
    "                out = transform_asff(df_raw)\n",
    "            elif src == \"OCSF\":\n",
    "                out = transform_ocsf(df_raw)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            out = out.withColumn(\"finding_id\", normalize_finding_id(F.col(\"finding_id\"))) \\\n",
    "                     .where(F.col(\"finding_id\").isNotNull())\n",
    "            canonical_dfs.append(out)\n",
    "\n",
    "        if not canonical_dfs:\n",
    "            print(f\"[SKIP] No valid findings after filtering for {company_id}\")\n",
    "            return (False, \"No valid findings\")\n",
    "\n",
    "        df_union = canonical_dfs[0]\n",
    "        for d in canonical_dfs[1:]:\n",
    "            df_union = df_union.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "        union_count = df_union.count()\n",
    "        print(f\"[TRANSFORM] Union rows: {union_count:,}\")\n",
    "\n",
    "        # Deduplicate\n",
    "        w = Window.partitionBy(\"finding_id\").orderBy(\n",
    "            F.col(\"finding_modified_time\").desc_nulls_last(),\n",
    "            F.col(\"_preference\").desc(),\n",
    "            F.col(\"_bronze_processed_time\").desc_nulls_last()\n",
    "        )\n",
    "\n",
    "        findings = (\n",
    "            df_union\n",
    "            .withColumn(\"_rn\", F.row_number().over(w))\n",
    "            .where(F.col(\"_rn\") == 1)\n",
    "            .drop(\"_rn\", \"_preference\", \"_bronze_processed_time\")\n",
    "        )\n",
    "\n",
    "        findings_count = findings.count()\n",
    "        print(f\"[TRANSFORM] Deduplicated findings: {findings_count:,}\")\n",
    "\n",
    "        # ============================================================\n",
    "        # APPLY CORRECT SEVERITY FROM REFERENCE TABLE\n",
    "        # ============================================================\n",
    "\n",
    "        if controls_ref_df is not None:\n",
    "            # Left join with reference table to get correct severity\n",
    "            findings = (\n",
    "                findings\n",
    "                .join(\n",
    "                    controls_ref_df.withColumnRenamed(\"severity\", \"ref_severity\"),\n",
    "                    on=\"control_id\",\n",
    "                    how=\"left\"\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"severity\",\n",
    "                    # Use reference severity if available, otherwise keep original\n",
    "                    F.when(F.col(\"ref_severity\").isNotNull(), F.lower(F.col(\"ref_severity\")))\n",
    "                     .otherwise(F.lower(F.col(\"severity\")))\n",
    "                )\n",
    "                .drop(\"ref_severity\")\n",
    "            )\n",
    "\n",
    "            print(f\"[SEVERITY] Applied reference table corrections (control_id -> severity)\")\n",
    "\n",
    "        else:\n",
    "            print(f\"[SEVERITY] Using severity from source data (no reference table)\")\n",
    "\n",
    "        # ============================================================\n",
    "        # IN-MEMORY → OUTPUT: Aggregate and Write\n",
    "        # ============================================================\n",
    "\n",
    "        # Aggregate to control-level\n",
    "        controls = aggregate_findings_to_controls(findings)\n",
    "\n",
    "        active_findings = findings.where(F.upper(F.col(\"finding_status\")) != F.lit(\"SUPPRESSED\"))\n",
    "        active_count = active_findings.count()\n",
    "        suppressed_count = findings_count - active_count\n",
    "        print(f\"[AGGREGATE] Active findings: {active_count:,}\")\n",
    "        print(f\"[AGGREGATE] Suppressed findings: {suppressed_count:,}\")\n",
    "\n",
    "        # Aggregate to account/region summary\n",
    "        standards_summary_df = aggregate_account_region_summary(controls, company_id, cf_processed_time)\n",
    "\n",
    "        standards_summary_count = standards_summary_df.count()\n",
    "        print(f\"[AGGREGATE] Standards summary rows: {standards_summary_count}\")\n",
    "\n",
    "        if standards_summary_count > 0:\n",
    "            # Clear old data (1-day retention strategy)\n",
    "            spark.sql(f\"TRUNCATE TABLE {standards_summary_tbl}\")\n",
    "\n",
    "            # Write new data\n",
    "            standards_summary_df.write.mode(\"append\").insertInto(standards_summary_tbl)\n",
    "\n",
    "            print(\"[WRITE] Standards summary table updated (TRUNCATE + Append)\")\n",
    "\n",
    "            # ============================================================\n",
    "            # ACCOUNT-LEVEL COMPLIANCE SUMMARY\n",
    "            # ============================================================\n",
    "\n",
    "            # Aggregate regional data to account level\n",
    "            account_summary_tbl = f\"{catalog_name}.{company_id}.aws_account_compliance_summary\"\n",
    "\n",
    "            account_summary = (\n",
    "                standards_summary_df\n",
    "                .groupBy(\"company_id\", \"account_id\")\n",
    "                .agg(\n",
    "                    F.sum(\"total_rules\").cast(\"int\").alias(\"total_rules\"),\n",
    "                    F.sum(\"total_passed\").cast(\"int\").alias(\"total_passed\"),\n",
    "                    F.first(\"cf_processed_time\").alias(\"cf_processed_time\")\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"score\",\n",
    "                    F.round(\n",
    "                        F.when(F.col(\"total_rules\") > 0, (F.col(\"total_passed\") / F.col(\"total_rules\")) * 100)\n",
    "                         .otherwise(0.0),\n",
    "                        7\n",
    "                    )\n",
    "                )\n",
    "                .select(\n",
    "                    \"company_id\",\n",
    "                    \"cf_processed_time\",\n",
    "                    \"account_id\",\n",
    "                    \"score\",\n",
    "                    \"total_rules\",\n",
    "                    \"total_passed\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            account_count = account_summary.count()\n",
    "            print(f\"[AGGREGATE] Account-level summary rows: {account_count}\")\n",
    "\n",
    "            if account_count > 0:\n",
    "                # Create account summary table if it doesn't exist\n",
    "                spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {account_summary_tbl} (\n",
    "                  company_id STRING COMMENT 'データが属する企業のID',\n",
    "                  cf_processed_time TIMESTAMP COMMENT '集計日 (Jobの実行日時)',\n",
    "                  account_id STRING COMMENT 'アカウントID',\n",
    "                  score FLOAT COMMENT 'コントロールに基づく総合スコア (passed_rules / total_rules * 100)',\n",
    "                  total_rules INT COMMENT '適用コントロール総数 (passed + failed + unknown)',\n",
    "                  total_passed INT COMMENT '合格コントロール数'\n",
    "                )\n",
    "                USING DELTA\n",
    "                COMMENT 'Account-level compliance summary aggregated across all regions'\n",
    "                \"\"\")\n",
    "\n",
    "                # Clear old data (1-day retention strategy)\n",
    "                spark.sql(f\"TRUNCATE TABLE {account_summary_tbl}\")\n",
    "\n",
    "                # Write account summary\n",
    "                account_summary.write.mode(\"append\").insertInto(account_summary_tbl)\n",
    "\n",
    "                print(f\"[WRITE] Account summary table updated: {account_summary_tbl}\")\n",
    "\n",
    "        print(f\"[SUCCESS] Completed processing for {company_id}\")\n",
    "        return (True, \"Success\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {company_id}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return (False, str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45d37467-4234-43c3-9f0f-1fdf8b111acf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Execute Pipeline\n",
    "\n",
    "Process all companies sequentially and track results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f4c28d-4036-4352-8952-552cc8ef4021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXECUTE SEQUENTIAL PROCESSING\n",
    "# ============================================================\n",
    "\n",
    "# Track results\n",
    "successful_companies = []\n",
    "failed_companies = []\n",
    "skipped_companies = []\n",
    "\n",
    "for company_id in companies_to_process:\n",
    "    # Ensure output tables exist before processing\n",
    "    ensure_output_tables_exist(catalog_name, company_id)\n",
    "\n",
    "    success, message = process_company(\n",
    "        company_id,\n",
    "        catalog_name,\n",
    "        window_start_ts,\n",
    "        window_end_ts,\n",
    "        cf_processed_time\n",
    "    )\n",
    "\n",
    "    if success:\n",
    "        successful_companies.append(company_id)\n",
    "    elif message in [\"No bronze tables\", \"No data in window\", \"No valid findings\"]:\n",
    "        skipped_companies.append((company_id, message))\n",
    "    else:\n",
    "        failed_companies.append((company_id, message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1600f07-0a90-42fd-b326-75e04a98e8bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Summary Report\n",
    "\n",
    "Display processing results and raise exception if any failures occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e78dbda-d72d-40fc-82b5-b9f87dd4a852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY OUTPUT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECURITY HUB STANDARDS ETL PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Companies:    {len(companies_to_process)}\")\n",
    "print(f\"[SUCCESS]           {len(successful_companies)}\")\n",
    "print(f\"[SKIPPED]           {len(skipped_companies)}\")\n",
    "print(f\"[FAILED]            {len(failed_companies)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if successful_companies:\n",
    "    print(f\"\\n[SUCCESS] COMPLETED COMPANIES ({len(successful_companies)}):\")\n",
    "    for comp_id in successful_companies:\n",
    "        print(f\"  - {comp_id}\")\n",
    "\n",
    "if skipped_companies:\n",
    "    print(f\"\\n[SKIPPED] COMPANIES ({len(skipped_companies)}):\")\n",
    "    for comp_id, reason in skipped_companies:\n",
    "        print(f\"  - [{comp_id}] {reason}\")\n",
    "\n",
    "if failed_companies:\n",
    "    print(f\"\\n[FAILED] COMPANIES ({len(failed_companies)}):\")\n",
    "    for comp_id, error in failed_companies:\n",
    "        print(f\"  - [{comp_id}] {error[:150]}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    raise Exception(f\"Pipeline failed for {len(failed_companies)} company(ies). See error details above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[SUCCESS] SECURITY HUB STANDARDS ETL COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_to_gold_v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}