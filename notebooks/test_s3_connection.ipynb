{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cc97ee",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set up S3 bucket path and test parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# S3 configuration (same as production)\n",
    "s3_base_path = \"s3://dev-cf-databricks-catalog-bucket/dev/dashboard/compliance\"\n",
    "s3_test_path = f\"{s3_base_path}/_connection_test\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"S3 CONNECTION TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"S3 Base Path:  {s3_base_path}\")\n",
    "print(f\"Test Path:     {s3_test_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ff4e9",
   "metadata": {},
   "source": [
    "## 2. Test 1: List S3 Bucket Contents\n",
    "\n",
    "Verify read access to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 1: LIST S3 BUCKET CONTENTS (READ ACCESS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # List files in the base path\n",
    "    files = dbutils.fs.ls(s3_base_path)\n",
    "\n",
    "    print(f\"‚úÖ SUCCESS: Listed {len(files)} items in {s3_base_path}\")\n",
    "    print(\"\\nContents:\")\n",
    "    for f in files[:10]:  # Show first 10 items\n",
    "        file_type = \"DIR \" if f.isDir() else \"FILE\"\n",
    "        size_mb = f.size / (1024 * 1024) if not f.isDir() else 0\n",
    "        print(f\"  [{file_type}] {f.name:50s} ({size_mb:.2f} MB)\")\n",
    "\n",
    "    if len(files) > 10:\n",
    "        print(f\"  ... and {len(files) - 10} more items\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: Cannot list S3 bucket\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd79fb",
   "metadata": {},
   "source": [
    "## 3. Test 2: Write Test File to S3\n",
    "\n",
    "Verify write access and CSV export functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8959dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: WRITE TEST CSV TO S3 (WRITE ACCESS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Create a small test DataFrame\n",
    "    test_data = [\n",
    "        (\"test_company_001\", \"2024-12-26\", \"123456789012\", \"us-east-1\", 85.5, 120, 103),\n",
    "        (\"test_company_001\", \"2024-12-26\", \"123456789012\", \"us-west-2\", 90.0, 120, 108),\n",
    "        (\"test_company_002\", \"2024-12-26\", \"987654321098\", \"us-east-1\", 75.0, 100, 75),\n",
    "    ]\n",
    "\n",
    "    test_df = spark.createDataFrame(\n",
    "        test_data,\n",
    "        [\"company_id\", \"date\", \"account_id\", \"region_id\", \"score\", \"total_rules\", \"total_passed\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Created test DataFrame with {test_df.count()} rows\")\n",
    "    print(\"\\nTest data:\")\n",
    "    test_df.show(truncate=False)\n",
    "\n",
    "    # Write to S3 with same options as production\n",
    "    (test_df\n",
    "     .repartition(\"company_id\")\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"header\", \"true\")\n",
    "     .option(\"compression\", \"gzip\")\n",
    "     .option(\"maxRecordsPerFile\", 200000)\n",
    "     .partitionBy(\"company_id\", \"date\")\n",
    "     .csv(s3_test_path))\n",
    "\n",
    "    print(f\"‚úÖ SUCCESS: Test CSV written to {s3_test_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: Cannot write to S3\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe4926",
   "metadata": {},
   "source": [
    "## 4. Test 3: Read Back Test File\n",
    "\n",
    "Verify written data can be read back correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25003bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: READ BACK TEST FILE (VERIFY WRITE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Read back the test file\n",
    "    read_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(s3_test_path)\n",
    "    )\n",
    "\n",
    "    row_count = read_df.count()\n",
    "    print(f\"‚úÖ SUCCESS: Read {row_count} rows from S3\")\n",
    "\n",
    "    print(\"\\nRead data:\")\n",
    "    read_df.orderBy(\"company_id\", \"region_id\").show(truncate=False)\n",
    "\n",
    "    # Verify data integrity\n",
    "    if row_count == 3:\n",
    "        print(\"‚úÖ Data integrity verified: Row count matches\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Expected 3 rows, got {row_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: Cannot read from S3\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772ab23",
   "metadata": {},
   "source": [
    "## 5. Test 4: Verify S3 Folder Structure\n",
    "\n",
    "Check that partitioning creates the expected folder hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6597f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: VERIFY S3 FOLDER STRUCTURE (PARTITIONING)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # List top-level (should see company_id= folders)\n",
    "    company_folders = dbutils.fs.ls(s3_test_path)\n",
    "    print(f\"‚úÖ Found {len(company_folders)} company folders:\")\n",
    "    for folder in company_folders:\n",
    "        if folder.name.startswith(\"company_id=\"):\n",
    "            print(f\"  - {folder.name}\")\n",
    "\n",
    "            # List date folders within each company\n",
    "            date_folders = dbutils.fs.ls(folder.path)\n",
    "            for date_folder in date_folders:\n",
    "                if date_folder.name.startswith(\"date=\"):\n",
    "                    print(f\"    - {date_folder.name}\")\n",
    "\n",
    "                    # List CSV files\n",
    "                    csv_files = dbutils.fs.ls(date_folder.path)\n",
    "                    csv_count = len([f for f in csv_files if f.name.endswith(\".csv.gz\")])\n",
    "                    print(f\"      ‚Üí {csv_count} CSV file(s)\")\n",
    "\n",
    "    print(\"\\n‚úÖ SUCCESS: Folder structure matches expected pattern:\")\n",
    "    print(\"   s3://.../company_id=xxx/date=YYYY-MM-DD/*.csv.gz\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: Folder structure verification failed\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eacd9f",
   "metadata": {},
   "source": [
    "## 6. Test 5: Test Parallel Write with Repartition\n",
    "\n",
    "Verify that repartition creates separate files per company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c61d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 5: PARALLEL WRITE VERIFICATION (REPARTITION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Create test data with multiple companies\n",
    "    large_test_data = []\n",
    "    for i in range(1, 6):  # 5 companies\n",
    "        company_id = f\"test_company_{i:03d}\"\n",
    "        for j in range(1, 11):  # 10 rows per company\n",
    "            large_test_data.append((\n",
    "                company_id,\n",
    "                \"2024-12-26\",\n",
    "                f\"12345678{i:04d}\",\n",
    "                f\"region-{j}\",\n",
    "                85.0 + i,\n",
    "                100,\n",
    "                85 + i\n",
    "            ))\n",
    "\n",
    "    large_df = spark.createDataFrame(\n",
    "        large_test_data,\n",
    "        [\"company_id\", \"date\", \"account_id\", \"region_id\", \"score\", \"total_rules\", \"total_passed\"]\n",
    "    )\n",
    "\n",
    "    total_rows = large_df.count()\n",
    "    company_count = large_df.select(\"company_id\").distinct().count()\n",
    "    print(f\"Created DataFrame: {total_rows} rows, {company_count} companies\")\n",
    "\n",
    "    # Repartition by company_id (Serverless auto-scales partitions)\n",
    "    repartitioned_df = large_df.repartition(\"company_id\")\n",
    "    print(f\"Repartitioned by company_id (Databricks Serverless auto-scales)\")\n",
    "\n",
    "    # Write with parallel processing\n",
    "    s3_parallel_test_path = f\"{s3_base_path}/_parallel_test\"\n",
    "\n",
    "    (repartitioned_df\n",
    "     .write\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"header\", \"true\")\n",
    "     .option(\"compression\", \"gzip\")\n",
    "     .partitionBy(\"company_id\", \"date\")\n",
    "     .csv(s3_parallel_test_path))\n",
    "\n",
    "    print(f\"‚úÖ SUCCESS: Parallel write completed to {s3_parallel_test_path}\")\n",
    "\n",
    "    # Verify each company has its own folder\n",
    "    company_folders = dbutils.fs.ls(s3_parallel_test_path)\n",
    "    company_folder_count = len([f for f in company_folders if f.name.startswith(\"company_id=\")])\n",
    "\n",
    "    print(f\"\\n‚úÖ Verification: Created {company_folder_count} company folders\")\n",
    "    if company_folder_count == company_count:\n",
    "        print(\"‚úÖ PASSED: Each company has separate S3 folder (parallel write confirmed)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Expected {company_count} folders, found {company_folder_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FAILED: Parallel write test failed\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709285b",
   "metadata": {},
   "source": [
    "## 7. Cleanup: Remove Test Files\n",
    "\n",
    "Clean up test files from S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7104fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEANUP: REMOVING TEST FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Remove test files\n",
    "    dbutils.fs.rm(s3_test_path, recurse=True)\n",
    "    print(f\"‚úÖ Removed: {s3_test_path}\")\n",
    "\n",
    "    dbutils.fs.rm(f\"{s3_base_path}/_parallel_test\", recurse=True)\n",
    "    print(f\"‚úÖ Removed: {s3_base_path}/_parallel_test\")\n",
    "\n",
    "    print(\"\\n‚úÖ CLEANUP COMPLETE\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Cleanup failed (files may still exist)\")\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12bbd07",
   "metadata": {},
   "source": [
    "## 8. Summary Report\n",
    "\n",
    "Final summary of all S3 connection tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e04993",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"S3 CONNECTION TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ TEST 1: List S3 bucket contents (READ)\")\n",
    "print(\"‚úÖ TEST 2: Write CSV to S3 (WRITE)\")\n",
    "print(\"‚úÖ TEST 3: Read back CSV from S3 (VERIFY)\")\n",
    "print(\"‚úÖ TEST 4: Verify folder structure (PARTITIONING)\")\n",
    "print(\"‚úÖ TEST 5: Parallel write with repartition (PERFORMANCE)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéâ ALL TESTS PASSED!\")\n",
    "print(\"\\nS3 connection is working correctly:\")\n",
    "print(f\"  - Read access: ‚úÖ\")\n",
    "print(f\"  - Write access: ‚úÖ\")\n",
    "print(f\"  - Compression (gzip): ‚úÖ\")\n",
    "print(f\"  - Partitioning (company_id/date): ‚úÖ\")\n",
    "print(f\"  - Parallel writes (.repartition): ‚úÖ\")\n",
    "print(\"\\n‚úÖ Ready for production deployment!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
