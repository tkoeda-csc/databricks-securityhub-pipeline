{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a520cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "database_name = \"cloudfastener\"\n",
    "schema_name = \"reference\"\n",
    "table_name = \"securityhub_controls\"\n",
    "full_table_name = f\"{database_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "# For Workspace files, use relative path (same directory as notebook)\n",
    "json_file_path = \"securityhub_controls.json\"\n",
    "\n",
    "print(f\"Target table: {full_table_name}\")\n",
    "print(f\"Source file: {json_file_path}\")\n",
    "print(\"NOTE: JSON file should be in the same Workspace directory as this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3257f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure database and schema exist\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {database_name}.{schema_name}\")\n",
    "\n",
    "print(f\"✓ Database and schema verified: {database_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table schema\n",
    "schema = StructType([\n",
    "    StructField(\"control_id\", StringType(), nullable=False),\n",
    "    StructField(\"severity\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "print(\"✓ Schema defined:\")\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file from Workspace - use Python to read, then create DataFrame\n",
    "import json as json_lib\n",
    "\n",
    "# Read JSON file using Python (works for Workspace files)\n",
    "with open(json_file_path, 'r') as f:\n",
    "    json_data = json_lib.load(f)\n",
    "\n",
    "# Create DataFrame from JSON data\n",
    "df = spark.createDataFrame(json_data, schema=schema)\n",
    "\n",
    "# Show sample data\n",
    "print(f\"✓ Loaded {df.count()} control records\")\n",
    "print(\"\\nSample data:\")\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\nData Quality Checks:\")\n",
    "print(f\"- Total records: {df.count()}\")\n",
    "print(f\"- Distinct control_ids: {df.select('control_id').distinct().count()}\")\n",
    "print(f\"- Null control_ids: {df.filter(df.control_id.isNull()).count()}\")\n",
    "print(f\"- Null severities: {df.filter(df.severity.isNull()).count()}\")\n",
    "\n",
    "# Show severity distribution\n",
    "print(\"\\nSeverity Distribution:\")\n",
    "df.groupBy(\"severity\").count().orderBy(\"severity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop table if exists (for clean rebuild)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "print(f\"✓ Dropped existing table (if any): {full_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Delta table\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(full_table_name)\n",
    "\n",
    "print(f\"✓ Table created successfully: {full_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e772c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add table comments for documentation\n",
    "spark.sql(f\"\"\"\n",
    "    COMMENT ON TABLE {full_table_name}\n",
    "    IS 'Reference table for AWS Security Hub control ID to severity mappings.\n",
    "    Used in bronze_to_gold pipeline for correct severity attribution.'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"ALTER TABLE {full_table_name} CHANGE COLUMN control_id COMMENT 'Security Hub control identifier (e.g., ACM.1, S3.1)'\")\n",
    "spark.sql(f\"ALTER TABLE {full_table_name} CHANGE COLUMN severity COMMENT 'Control severity level: LOW, MEDIUM, HIGH, or CRITICAL'\")\n",
    "\n",
    "print(\"✓ Table comments added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eab9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify table creation and contents\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TABLE VERIFICATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Show table properties\n",
    "print(\"Table Description:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {full_table_name}\").show(truncate=False)\n",
    "\n",
    "# Count records\n",
    "record_count = spark.table(full_table_name).count()\n",
    "print(f\"\\n✓ Total records in table: {record_count}\")\n",
    "\n",
    "# Sample records\n",
    "print(\"\\nSample records from table:\")\n",
    "spark.table(full_table_name).show(20, truncate=False)\n",
    "\n",
    "# Show some specific examples\n",
    "print(\"\\nExample lookups:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT control_id, severity\n",
    "    FROM {full_table_name}\n",
    "    WHERE control_id IN ('ACM.1', 'S3.1', 'EC2.1', 'IAM.1')\n",
    "    ORDER BY control_id\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary report\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"✓ Table Name: {full_table_name}\")\n",
    "print(f\"✓ Record Count: {record_count}\")\n",
    "print(f\"✓ Columns: control_id (STRING), severity (STRING)\")\n",
    "print(f\"✓ Table Format: Delta\")\n",
    "print(f\"\\n✓ Table is ready for use in bronze_to_gold_v2 pipeline!\")\n",
    "print(f\"\\nUsage example in your pipeline:\")\n",
    "print(f\"\"\"\n",
    "# Join with controls reference table to get correct severity\n",
    "controls_ref = spark.table(\"{full_table_name}\")\n",
    "df_with_severity = df.join(\n",
    "    controls_ref,\n",
    "    df.control_id == controls_ref.control_id,\n",
    "    \"left\"\n",
    ").select(\n",
    "    df.*,\n",
    "    controls_ref.severity.alias(\"correct_severity\")\n",
    ")\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
